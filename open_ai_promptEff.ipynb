{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe310694",
   "metadata": {},
   "source": [
    "### Imports and Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "380efac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c5688",
   "metadata": {},
   "source": [
    "### Define model versions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2cfcaa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_1 = \"gpt-4o-2024-08-06\" # For generating model responses\n",
    "MODEL_2 = \"gpt-4o-2024-08-06\" # For evaluating prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79810fae",
   "metadata": {},
   "source": [
    "### Function to Generate Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03f2774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(prompt: str, model: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response from the given model for the specified prompt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_completion_tokens=1000  # You may adjust based on expected output length\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in generate_response]: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ec494",
   "metadata": {},
   "source": [
    "### Complexity Dimension Alignment Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "095c10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifies if the user prompt reflects the complexity dimensions expected by the question.\n",
    "# Returns structured JSON with identified dimensions and a relevance score.\n",
    "def check_complexity_criteria(user_prompt: str, question: str, model):\n",
    "    complexity_prompt = f\"\"\"\n",
    "You are an expert evaluator. Your job is to verify whether the complexity dimensions mentioned in the QUESTION are clearly implemented in the USER PROMPT.\n",
    "\n",
    "### DIMENSIONS CHECKLIST :\n",
    "\n",
    "### 1. Nested / Multi‐Step Instructions\n",
    "**Definition**: The prompt requires the model to perform multiple subtasks in a specified order (e.g., “Step 1: …, Step 2: …, Step 3: …”).\n",
    "**Key Indicators**:\n",
    "  - Are there explicit numbered or indented steps?\n",
    "  - Does one instruction logically precede another?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Conflicting Instructions\n",
    "**Definition**: The prompt imposes two or more constraints that cannot all be satisfied simultaneously (e.g., “Write a summary in under 50 words but include five examples”).\n",
    "**Key Indicators**:\n",
    "  - Do two (or more) requirements directly contradict each other?\n",
    "  - Is the prompt forcing the model to choose or partially satisfy conflicting directives?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inter‐Dependent Constraints\n",
    "**Definition**: One instruction becomes active only if another condition is met (e.g., “If the text contains more than ten technical terms, define each; otherwise, just list them”).\n",
    "**Key Indicators**:\n",
    "  - Is there an explicit “if‐then” or “only when” dependency?\n",
    "  - Does satisfying the second instruction depend on the first instruction’s outcome?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Edge‐Case Handling\n",
    "**Definition**: The prompt instructs the model to explicitly admit when required information is missing or when an edge case arises (e.g., “If the input lacks dates, respond ‘Insufficient data’”).\n",
    "**Key Indicators**:\n",
    "  - Does the prompt say “If X is absent or unclear, do not guess”?\n",
    "  - Are instructions provided for how to behave if required data is missing?\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ambiguity Resolution\n",
    "**Definition**: The prompt introduces ambiguous terms or references and instructs the model to clarify or handle them (e.g., “The word ‘bank’ could mean financial institution or riverbank—state both interpretations”).\n",
    "**Key Indicators**:\n",
    "  - Are ambiguous words or phrases flagged?\n",
    "  - Does the prompt ask “If ambiguous, explain interpretations”?\n",
    "---\n",
    "\n",
    "### 6. Domain Fusion\n",
    "**Definition**: The prompt fuses two or more specialized domains into one task (e.g., “Analyze the legal contract’s economic impact using statistical models”).\n",
    "**Key Indicators**:\n",
    "  - Are at least two distinct fields explicitly mentioned?\n",
    "  - Does the prompt require coherent integration of knowledge from both domains?\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Multi‐Source/Modal Analysis\n",
    "**Definition**: The model must reason over multiple distinct inputs (e.g., two text passages, text plus described visuals) and synthesize across them.\n",
    "**Key Indicators**:\n",
    "  - Are there at least two distinct “sources” described?\n",
    "  - Does the prompt instruct the model to integrate information across those sources?\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Hypothetical / Counterfactual Reasoning\n",
    "**Definition**: The prompt poses a scenario contrary to known facts or purely hypothetical (e.g., “If gravity were inverted, describe consequences for river flow”).\n",
    "**Key Indicators**:\n",
    "  - Does it present an “as if” scenario explicitly defying reality?\n",
    "  - Are you instructed to treat that scenario as true and reason within it?\n",
    "\n",
    "---\n",
    "Be accurate, thorough, and follow instructions step-by-step.\n",
    "\n",
    "You MUST follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### STEP 1: IDENTIFY DIMENSIONS IN QUESTION\n",
    "\n",
    "From the QUESTION text below, identify all the complexity dimensions it expects the prompt to include.\n",
    "Only consider a dimension to be expected if:\n",
    "- Its core definition or indicators are described or implied.\n",
    "\n",
    "---\n",
    "\n",
    "### STEP 2: VERIFY DIMENSIONS IN USER PROMPT\n",
    "\n",
    "Next, check which of the above dimensions are **actually present** in the USER PROMPT.\n",
    "\n",
    "Only mark a dimension as present if:\n",
    "- The USER PROMPT shows clear evidence of fulfilling the definition or key indicators of that dimension.\n",
    "- You can justify its presence using the specific examples listed in the DIMENSION CHECKLIST.\n",
    "\n",
    "---\n",
    "\n",
    "### STEP 3: SCORE AND EVALUATE\n",
    "\n",
    "Use this formula to compute the score:\n",
    "\n",
    "    Score = (number of dimensions present in the USER PROMPT) / (number of dimensions required by the QUESTION)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Very Important and give more weightage -->>***Please return your answer *strictly in JSON* using the format below — no markdown, no extra explanation:***\n",
    "### VERY IMPORTANT:\n",
    "- You must return the response **strictly in valid JSON**.\n",
    "- Do NOT use Markdown.\n",
    "- Do NOT include any preamble, footnotes, or commentary—just return the raw JSON object.\n",
    "\n",
    "### STEP 4: OUTPUT STRICTLY IN THIS JSON FORMAT:\n",
    "{{\n",
    "  \"Dimensions in question\": [\"Hypothetical / Counterfactual Reasoning\", \"Ambiguity\", \"Multi‐Source/Modal Analysis\", \"Domain Fusion\", \"Edge‐Case Handling\", ...],\n",
    "  \"TotalDimensions in question\": <count>,\n",
    "  \"Dimensions in prompt\": [\"Ambiguity\", \"Multi‐Source/Modal Analysis\", \"Domain Fusion\"...],\n",
    "  \"TotalDimensions in prompt\": <count>,\n",
    "  \"Score\": <float between 0 and 1, rounded to 2 decimals...(TotalDimensions in prompt)/(TotalDimensions in question)>\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "### INPUTS:\n",
    "- USER PROMPT: \"{user_prompt}\"\n",
    "- QUESTION: \"{question}\"\n",
    "\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with a valid JSON object.\"},\n",
    "                {\"role\": \"user\", \"content\": complexity_prompt}\n",
    "            ],\n",
    "            max_completion_tokens=512\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in complexity check]: {e}\"\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeded1d",
   "metadata": {},
   "source": [
    "### LLM-Based Prompt Challenge Evaluator (Complexity_Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60ee8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses an LLM-as-judge to assess how effectively the USER PROMPT challenged the model, based on dimension-wise failure analysis of the MODEL RESPONSE.\n",
    "def judge_response(user_prompt, model_response, total_dimensions, model):\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert language model evaluator.\n",
    "\n",
    "Your task is to assess how effectively a USER PROMPT challenged a MODEL, based on its RESPONSE. You will do this by analyzing the model’s performance across eight cognitive and reasoning dimensions.\n",
    "The more the model fails, the more effective and challenging the prompt is considered.\n",
    "\n",
    "Then, provide a **Summary** including:\n",
    "**ApplicableDimensions**: List[dimensions with non-null scores]\n",
    "**normalizedScore**: <sum of normalized scores>,\n",
    "**OverallScore**: <normalizedScore / D>,\n",
    "**Explanation: \"Summarize the model's performance across dimensions, highlighting where it failed and how that shows the prompt was strong.\",\n",
    "**FailureTags (if applicable): [\"Omission\", \"Ambiguity\", \"Conflict\", \"Shallow Reasoning\", ...],\n",
    "\n",
    "--- \n",
    "\n",
    "Use the following dimensions to guide your evaluation:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Nested / Multi‐Step Instructions\n",
    "**Definition**: The prompt requires the model to perform multiple subtasks in a specified order (e.g., “Step 1: …, Step 2: …, Step 3: …”).\n",
    "**Key Indicators**:\n",
    "  - Are there explicit numbered or indented steps?\n",
    "  - Does one instruction logically precede another?\n",
    "**Prompt Ratings** (based on model performance):\n",
    "  - Bad (8–10): The model failed — skipped steps, did them out of order, or gave shallow detail. Prompt is strong.\n",
    "  - Average (4–7): The model made small mistakes — minor ordering errors or partial steps. Prompt is moderately challenging.\n",
    "  - Good (0–3): The model followed all steps correctly. Prompt did not challenge the model.\n",
    "  - N/A: The prompt does not require multiple subtasks (e.g., a straightforward factual query).\n",
    "---\n",
    "\n",
    "### 2. Conflicting Instructions\n",
    "**Definition**: The prompt imposes two or more constraints that cannot all be satisfied simultaneously (e.g., “Write a summary in under 50 words but include five examples”).\n",
    "**Key Indicators**:\n",
    "  - Do two (or more) requirements directly contradict each other?\n",
    "  - Is the prompt forcing the model to choose or partially satisfy conflicting directives?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model failed to resolve the conflict, picked blindly, or gave up. Prompt exposed limitations.\n",
    "  - Average (4–7): The model noticed the conflict but handled it only partially or with weak justification. Prompt had mild challenge.\n",
    "  - Good (0–3): The model resolved the conflict gracefully and explained the trade-off. Prompt was too easy.\n",
    "  - N/A: The prompt has no conflicting requirements (e.g., simply “List all prime numbers under 100”).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inter‐Dependent Constraints\n",
    "**Definition**: One instruction becomes active only if another condition is met (e.g., “If the text contains more than ten technical terms, define each; otherwise, just list them”).\n",
    "**Key Indicators**:\n",
    "  - Is there an explicit “if‐then” or “only when” dependency?\n",
    "  - Does satisfying the second instruction depend on the first instruction’s outcome?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model ignored the conditional logic or applied it incorrectly. Prompt was logically challenging.\n",
    "  - Average (4–7): The model partially followed the conditions but missed nuances. Prompt was somewhat effective.\n",
    "  - Good (0–3): The model applied conditions precisely. Prompt failed to test dependency logic.\n",
    "  - N/A: No conditional instructions present (e.g., “Translate this sentence into Spanish”).\n",
    "---\n",
    "\n",
    "### 4. Edge‐Case Handling\n",
    "**Definition**: The prompt instructs the model to explicitly admit when required information is missing or when an edge case arises (e.g., “If the input lacks dates, respond ‘Insufficient data’”).\n",
    "**Key Indicators**:\n",
    "  - Does the prompt say “If X is absent or unclear, do not guess”?\n",
    "  - Are instructions provided for how to behave if required data is missing?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model hallucinated, guessed incorrectly, or failed to handle edge cases. Prompt forced the model to struggle.\n",
    "  - Average (4–7): The model sometimes handled edge cases, but not consistently. Prompt added some challenge.\n",
    "  - Good (0–3): The model correctly followed instructions for all missing or unclear input. Prompt lacked edge-case difficulty.\n",
    "  - N/A: The prompt’s context guarantees all information is present (e.g., “Compute the sum of the following five numbers”).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ambiguity Resolution\n",
    "**Definition**: The prompt introduces ambiguous terms or references and instructs the model to clarify or handle them (e.g., “The word ‘bank’ could mean financial institution or riverbank—state both interpretations”).\n",
    "**Key Indicators**:\n",
    "  - Are ambiguous words or phrases flagged?\n",
    "  - Does the prompt ask “If ambiguous, explain interpretations”?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model picked a single meaning without explanation or failed to identify the ambiguity. Prompt successfully revealed limits.\n",
    "  - Average (4–7): The model noticed ambiguity but didn’t explain well or fully. Prompt was moderately effective.\n",
    "  - Good (0–3): The model clearly explained all interpretations and justified its choice. Prompt wasn’t challenging.\n",
    "  - N/A: No ambiguity in the prompt (e.g., “List the first ten Fibonacci numbers”).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Domain Fusion\n",
    "**Definition**: The prompt fuses two or more specialized domains into one task (e.g., “Analyze the legal contract’s economic impact using statistical models”).\n",
    "**Key Indicators**:\n",
    "  - Are at least two distinct fields explicitly mentioned?\n",
    "  - Does the prompt require coherent integration of knowledge from both domains?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model failed to link domains, missed one entirely, or showed shallow understanding. Prompt was deeply integrative.\n",
    "  - Average (4–7): Model partially integrated domains or showed uneven depth. Prompt was somewhat integrative.\n",
    "  - Good (0–3): Model combined domains effortlessly and with accuracy. Prompt wasn’t cognitively complex.\n",
    "  - N/A: Task focuses on a single domain (e.g., “Explain basic thermodynamics”).\n",
    "---\n",
    "\n",
    "### 7. Multi‐Source/Modal Analysis\n",
    "**Definition**: The model must reason over multiple distinct inputs (e.g., two text passages, text plus described visuals) and synthesize across them.\n",
    "**Key Indicators**:\n",
    "  - Are there at least two distinct “sources” described?\n",
    "  - Does the prompt instruct the model to integrate information across those sources?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model ignored one or more sources or failed to synthesize. Prompt challenged multi-input reasoning.\n",
    "  - Average (4–7): The model mentioned multiple sources but lacked coherent synthesis. Prompt was modestly challenging.\n",
    "  - Good (0–3): The model used all sources well and produced a coherent result. Prompt failed to create input complexity.\n",
    "  - N/A: Task provides a single input (e.g., “Summarize this paragraph”).\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Hypothetical / Counterfactual Reasoning\n",
    "**Definition**: The prompt poses a scenario contrary to known facts or purely hypothetical (e.g., “If gravity were inverted, describe consequences for river flow”).\n",
    "**Key Indicators**:\n",
    "  - Does it present an “as if” scenario explicitly defying reality?\n",
    "  - Are you instructed to treat that scenario as true and reason within it?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model reverted to real-world facts, ignored the hypothetical, or answered shallowly. Prompt tested imaginative reasoning.\n",
    "  - Average (4–7): The model stayed in the scenario but didn’t explore implications fully. Prompt had partial depth.\n",
    "  - Good (0–3): The model reasoned thoroughly and stayed within the hypothetical frame. Prompt was not mentally taxing.\n",
    "  - N/A: No hypothetical scenario—task uses real‐world facts only.\n",
    "\n",
    "---\n",
    "\n",
    "Evaluate the prompt and the model response using these questions:\n",
    "\n",
    "1. Did the model satisfy all critical expectations of the prompt?\n",
    "2. If the model failed, was it because the prompt was poorly constructed, overly ambiguous, or lacked proper scaffolding?\n",
    "3. Alternatively, if the model output was shallow or incorrect despite a strong prompt, then the prompt was effective because it exposed model limitations.\n",
    "4. Does It Break Nested / Multi-Step Instructions?\n",
    "  Failure Modes:\n",
    "  - Omission: The model skips one or more required steps.\n",
    "  - Wrong Order: Completes steps out of sequence.\n",
    "  - Incomplete Detail: Does not give enough detail for a subtask labeled “explain in detail.”\n",
    "\n",
    "5. Does It Break Conflicting Instructions?\n",
    "  Failure Modes:\n",
    "  - Blind Obedience: Tries to satisfy both contradictory instructions fully (and thus fails both).\n",
    "  - Undisclosed Choice: Picks one constraint without acknowledging the conflict.\n",
    "  - Refusal Without Explanation: Says “I can’t” but does not explain why.\n",
    "\n",
    "6. Does It Break Inter-Dependent Constraints?\n",
    "  Failure Modes:\n",
    "  - Misconditional: Applies the second constraint unconditionally or fails to check the first.\n",
    "  - Partial Compliance: Applies conditions incorrectly.\n",
    "\n",
    "7. Does It Break Edge-Case Handling?\n",
    "  Failure Modes:\n",
    "  - Fabrication: Hallucinates missing data instead of admitting it’s missing.\n",
    "  - Incorrect Catch: Says “insufficient data” when data actually exists.\n",
    "\n",
    "8. Does It Break Ambiguity Resolution?\n",
    "  Failure Modes:\n",
    "  - Single Interpretation: Chooses one meaning without acknowledging alternatives.\n",
    "  - Ignores Request: Fails to clarify ambiguous term.\n",
    "\n",
    "9. Does It Break Domain Fusion?\n",
    "  Failure Modes:\n",
    "  - Domain Drop: Only addresses one domain, ignoring the other.\n",
    "  - Shallow Fusion: Mentions the second domain only superficially.\n",
    "\n",
    "10. Does It Break Multi-Source/Modal Analysis?\n",
    "  Failure Modes:\n",
    "  - Single-Source Focus: Uses only one input, ignoring others.\n",
    "  - Surface Integration: Mentions multiple inputs but does not synthesize.\n",
    "\n",
    "11. Does It Break Hypothetical / Counterfactual Reasoning?\n",
    "  Failure Modes:\n",
    "  - Reality Check: Refuses or reaffirms real-world facts instead of engaging in the scenario.\n",
    "  - Shallow Answer: Acknowledges hypothetical but does not explore implications.\n",
    "  \n",
    "---\n",
    "\n",
    "Generate Explanation of Evaluation (0–10) with Failure Tags and Dimension-Based Analysis\n",
    "You are an evaluator assessing how well a model handled a complex prompt. Based on the score (from 0 to 10), write a detailed explanation that:\n",
    "1. Explains Why the Response Got This Score\n",
    "  - Describe how the model handled or failed each applicable complexity dimension.\n",
    "  - Clearly justify where the response was strong, and where it broke—i.e., failed to satisfy intended challenges.\n",
    "  - If the score is below 9, mention specific failure tags and explain why they apply.\n",
    "\n",
    "2. Use These Dimensions and Failure Modes\n",
    "Below are the 8 core dimensions you must consider. For each one, if it broke, use the relevant failure mode(s) and incorporate the corresponding failure tag(s) in your explanation.\n",
    "\n",
    "2.1. Nested / Multi-Step Instructions\n",
    "Failure Modes:\n",
    "  - Omission: Skips required step(s)\n",
    "  - Wrong Order: Steps executed out of sequence\n",
    "  - Incomplete Detail: Step lacks required elaboration\n",
    "Failure Tag: Missed nested instruction\n",
    "\n",
    "2.2. Conflicting Instructions\n",
    "Failure Modes:\n",
    "  - Blind Obedience: Tries to satisfy both conflicts fully\n",
    "  - Undisclosed Choice: Picks one side silently\n",
    "  - Refusal Without Explanation: Says “can’t” with no reason\n",
    "Failure Tag: Conflict evasion\n",
    "\n",
    "\n",
    "2.3. Inter-Dependent Constraints\n",
    "Failure Modes:\n",
    "  - Misconditional: Fails to apply logic in correct order\n",
    "  - Partial Compliance: Misinterprets conditions\n",
    "Failure Tag: Conditional failure\n",
    "\n",
    "2.4. Edge-Case Handling\n",
    "Failure Modes:\n",
    "  - Fabrication: Hallucinates data instead of admitting absence\n",
    "  - Incorrect Catch: Says data is missing when it's present\n",
    "Failure Tag: Edge case miss\n",
    "\n",
    "2.5. Ambiguity Resolution\n",
    "Failure Modes:\n",
    "  - Single Interpretation: Picks one meaning without context\n",
    "  - Ignores Request: Fails to clarify ambiguous terms\n",
    "Failure Tag: Ambiguity unresolved\n",
    "\n",
    "2.6. Domain Fusion\n",
    "Failure Modes:\n",
    "  - Domain Drop: Ignores one domain entirely\n",
    "  - Shallow Fusion: Surface-level integration\n",
    "Failure Tag: Domain omission\n",
    "\n",
    "2.7. Multi-Source / Modal Analysis\n",
    "Failure Modes:\n",
    "  - Single-Source Focus: Ignores other inputs\n",
    "  - Surface Integration: Uses but doesn't connect sources\n",
    "Failure Tag: Source integration failure\n",
    "\n",
    "2.8. Hypothetical / Counterfactual Reasoning\n",
    "Failure Modes:\n",
    "  - Reality Check: Rejects scenario, defaults to facts\n",
    "  - Shallow Answer: Ignoled implication depth\n",
    "Failure Tag: Shallow hypothetical\n",
    "\n",
    "3. Format Your Explanation Like This\n",
    "  - Reference all relevant dimensions and failure tags.\n",
    "  - If the response is strong (score 9–10), describe which dimensions were handled well and why.\n",
    "Goal\n",
    "The explanation should provide a clear rationale for the evaluation score, tied to concrete dimension-based reasoning. It must help prompt engineers or model developers understand what failed, why it failed, and how it can improve.\n",
    "\n",
    "---\n",
    "\n",
    "##  SCORING ONLY ON APPLICABLE DIMENSIONS\n",
    "\n",
    "You must only evaluate and score those dimensions which are listed in the input as **Applicable Dimensions** — these are the ones explicitly implemented in the USER PROMPT. Do not evaluate or score dimensions not included in this list.\n",
    "\n",
    "Use this variable as your scoring base:\n",
    "\n",
    "**Applicable Dimensions (INPUT)** = `TOTAL DIMENSIONS` = Number of dimensions implemented in the user prompt.\n",
    "\n",
    "---\n",
    "\n",
    "##  EVALUATION PHILOSOPHY\n",
    "\n",
    "When the model performs flawlessly, the prompt is **less challenging**.  \n",
    "When the model stumbles or fails, the prompt is **more challenging and valuable**.\n",
    "\n",
    "---\n",
    "\n",
    "##  EVALUATION STEPS\n",
    "\n",
    "### STEP 1: Analyze Each of the 8 Dimensions Independently\n",
    "\n",
    "For each of the 8 complexity dimensions below, do the following:\n",
    "\n",
    "1. **Check if the dimension is applicable** to the USER PROMPT.  \n",
    "   If it's not relevant or wasn't invoked by the prompt → **Mark as N/A**.\n",
    "\n",
    "2. **Evaluate how the model performed on that dimension** using the guidance below.\n",
    "\n",
    "3. **Assign:\n",
    "   - A **qualitative rating**: `\"Good\"`, `\"Average\"`, `\"Bad\"`, or `\"N/A\"`\n",
    "   - A **normalized numeric score**: `0.3`, `0.6`, `1.0`, or `null`\n",
    "\n",
    "---\n",
    "\n",
    "##  SCORING SCALE — WHAT EACH SCORE MEANS\n",
    "\n",
    "###  \"Good\" (Score: 0–3)\n",
    "\n",
    "Model did everything correctly. The prompt failed to challenge the model.\n",
    "\n",
    "| Score | When to Use |\n",
    "|-------|-------------|\n",
    "| 0     | Model executed flawlessly and easily. Prompt was too simple or surface-level. |\n",
    "| 1     | Model showed no confusion or hesitation; prompt had little depth. |\n",
    "| 2     | Prompt had mild complexity, but the model handled it with no difficulty. |\n",
    "| 3     | Prompt tried to be complex, but the model overcame all hurdles cleanly. |\n",
    "\n",
    "→ Assign normalized score: **0.3** which is (3 divided by 10)\n",
    "\n",
    "---\n",
    "\n",
    "###  \"Average\" (Score: 4–7)\n",
    "\n",
    "Model had partial success. The prompt created some cognitive load, but not enough to reliably break it.\n",
    "\n",
    "| Score | When to Use |\n",
    "|-------|-------------|\n",
    "| 4     | Model made subtle errors, missed nuance, or gave vague reasoning. |\n",
    "| 5     | Model followed some steps correctly but skipped others. |\n",
    "| 6     | Model misinterpreted minor conditions or showed shallow reasoning. |\n",
    "| 7     | Model mostly succeeded but missed edge cases, conflict handling, or counterfactual depth. |\n",
    "\n",
    "→ Assign normalized score: **0.6** which is (6 divided by 10)\n",
    "\n",
    "---\n",
    "\n",
    "###  \"Bad\" (Score: 8–10)\n",
    "\n",
    "Model clearly failed. The prompt successfully induced failure, confusion, or shallow reasoning.\n",
    "\n",
    "| Score | When to Use |\n",
    "|-------|-------------|\n",
    "| 8     | Model omitted major elements or misread conditional logic. |\n",
    "| 9     | Model failed to integrate domains or synthesize across inputs. |\n",
    "| 10    | Model broke entirely: hallucinated, contradicted, fabricated, or refused the task. |\n",
    "\n",
    "→ Assign normalized score: **1.0** which is (10 divided by 10)\n",
    "\n",
    "---\n",
    "\n",
    "###  \"N/A\"\n",
    "\n",
    "The dimension was **not present** in the prompt or **not assessable** in the model response.\n",
    "\n",
    "→ Score: `null`  \n",
    "→ Exclude from normalizedScore and OverallScore calculations.\n",
    "\n",
    "---\n",
    "\n",
    "##  OVERALL SCORING LOGIC\n",
    "\n",
    "- `ApplicableDimensions` = list of dimensions in the prompt\n",
    "Let:\n",
    "- **D** = number of **non-N/A** (applicable) dimensions, given below as a input -\"TOTAL DIMENSIONS\" or length of ApplicableDimensions.  \n",
    "- **S** = sum of **normalized scores** for those dimensions\n",
    "\n",
    "Then:\n",
    "- **normalizedScore = S** (which is sum of each normalized score e.g. (3/10 + 6/10 + 10/10....))\n",
    "- **OverallScore = S / D**\n",
    "\n",
    "Higher `OverallScore` = more challenging and effective prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### VERY IMPORTANT:\n",
    "- You must return the response **strictly in valid JSON**.\n",
    "- Do NOT use Markdown.\n",
    "- Do NOT include any preamble, footnotes, or commentary—just return the raw JSON object.\n",
    "\n",
    "\"example_output\": \n",
    "{{\n",
    "  \"Nested / Multi‐Step Instructions\": {{\n",
    "    \"Qualitative\": \"Good\",\n",
    "    \"Score\": 8.5\n",
    "  }},\n",
    "  \"Conflicting Instructions\": {{\n",
    "    \"Qualitative\": \"Bad\",\n",
    "    \"Score\": 2.0\n",
    "  }},\n",
    "  \"Inter‐Dependent Constraints\": {{\n",
    "    \"Qualitative\": \"Average\",\n",
    "    \"Score\": 6.0\n",
    "  }},\n",
    "  \"Edge‐Case Handling\": {{\n",
    "    \"Qualitative\": \"N/A\",\n",
    "    \"Score\": null\n",
    "  }},\n",
    "  \"Ambiguity Resolution\": {{\n",
    "    \"Qualitative\": \"Bad\",\n",
    "    \"Score\": 3.0\n",
    "  }},\n",
    "  \"Domain Fusion\": {{\n",
    "    \"Qualitative\": \"Good\",\n",
    "    \"Score\": 9.0\n",
    "  }},\n",
    "  \"Multi‐Source/Modal Analysis\": {{\n",
    "    \"Qualitative\": \"N/A\",\n",
    "    \"Score\": null\n",
    "  }},\n",
    "  \"Hypothetical / Counterfactual Reasoning\": {{\n",
    "    \"Qualitative\": \"Average\",\n",
    "    \"Score\": 5.5\n",
    "  }},\n",
    "  \"ApplicableDimensions\": [\n",
    "    \"Nested / Multi‐Step Instructions\",\n",
    "    \"Conflicting Instructions\",\n",
    "    \"Inter‐Dependent Constraints\",\n",
    "    \"Ambiguity Resolution\",\n",
    "    \"Domain Fusion\",\n",
    "    \"Hypothetical / Counterfactual Reasoning\"\n",
    "  ],\n",
    "  \"normalizedScore\": 34.0,\n",
    "  \"OverallScore\": 5.67,\n",
    "  \"Explanation\": \"The prompt was strong in exposing model weaknesses through Conflicting Instructions (failure to resolve contradictions) and Ambiguity Resolution (inconsistent interpretations), demonstrating effective challenge design. Domain Fusion was handled well, indicating weaker challenge. Average performance in Inter-Dependent Constraints and Hypothetical Reasoning shows moderate effectiveness. N/A dimensions were correctly excluded.\",\n",
    "  \"FailureTags\": [\"Conflict\", \"Ambiguity\", \"Shallow Reasoning\"],\n",
    "  \"StrengthAreas\": [\"Conflicting Instructions\", \"Ambiguity Resolution\"],\n",
    "  \"ImprovementAreas\": [\"Domain Fusion\"]\n",
    "}}\n",
    "\n",
    "##  OUTPUT FORMAT (REQUIRED)\n",
    "\n",
    "Very Important and give more weightage -->>***Please return your answer *strictly in JSON* using the format below — no markdown, no extra explanation:***\n",
    "\n",
    "{{\n",
    "    \"Nested / Multi‐Step Instructions\": {{ \"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <float or null> }},\n",
    "    \"Conflicting Instructions\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Inter‐Dependent Constraints\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Edge‐Case Handling\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Ambiguity Resolution\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Domain Fusion\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Multi‐Source/Modal Analysis\": {{ \"Qualitative\": \"...\", \"Score\": .<float or null>.. }},\n",
    "    \"Hypothetical / Counterfactual Reasoning\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"ApplicableDimensions\": [\"List of non-N/A dimensions\"],\n",
    "    \"normalizedScore\": (float)<sum of normalized scores>,\n",
    "    \"OverallScore\": <float between 0 and 1, rounded to 2 decimals...(normalizedScore)/(length of ApplicableDimensions)>,\n",
    "    \"Explanation\": \"Summarize the model's performance across dimensions, highlighting where it failed and how that shows the prompt was strong.\",\n",
    "    \"FailureTags\": [\"Omission\", \"Ambiguity\", \"Conflict\", \"Shallow Reasoning\", ...],\n",
    "    \"StrengthAreas\": [\"Dimensions where the model failed (prompt was strong)\"],\n",
    "    \"ImprovementAreas\": [\"Dimensions where the model succeeded (prompt was weak)\"]\n",
    "}}\n",
    "\n",
    "\n",
    "USER PROMPT:\n",
    "{user_prompt}\n",
    "\n",
    "MODEL RESPONSE:\n",
    "{model_response}\n",
    "\n",
    "TOTAL DIMENSIONS:\n",
    "{total_dimensions}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with a valid JSON object.\"},\n",
    "                {\"role\": \"user\", \"content\": judge_prompt}\n",
    "            ],\n",
    "            max_completion_tokens=512\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in judge_response]: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d2650",
   "metadata": {},
   "source": [
    "### LLM-Based Prompt Effectiveness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb908368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function employs an LLM-as-judge to evaluate the effectiveness of the USER PROMPT through a dimension-wise failure analysis\n",
    "def prompteffectiveness(user_prompt, question, model):\n",
    "    prompteff = f\"\"\"\n",
    "You are an expert evaluator tasked with analyzing a USER PROMPT and a corresponding QUESTION using the following 7 dimensions. Your goal is to critically assess the prompt's quality and effectiveness based on these dimensions, scoring each dimension qualitatively and quantitatively, and then provide a detailed summary.\n",
    "\n",
    "You will evaluate the prompt and question across **7 dimensions**, rating each as:\n",
    "- \"Good\": Strong performance in this aspect\n",
    "- \"Average\": Moderate or partial success\n",
    "- \"Bad\": Weakness or failure\n",
    "- \"N/A\": Not applicable to this prompt/response\n",
    "\n",
    "**Scoring Criteria**: A number between 0 and 10, where:\n",
    "  - **Good**: 8-10 points (8=solid good, 9=very good, 10=excellent)\n",
    "  - **Average**: 4-7 points (4-5=below average, 6-7=above average)\n",
    "  - **Bad**: 0-3 points (0=complete failure, 1-2=poor, 3=weak)\n",
    "  - **N/A**: No score assigned (excluded from overall average calculation)\n",
    "  \n",
    "Then, provide a **Summary** including:\n",
    "**OverallScore: A number between 0 and 10 (rounded to two decimal places), calculated as the average of all dimension scores\n",
    "  - Sum all numerical scores from applicable dimensions\n",
    "  - Divide by number of applicable dimensions (excluding N/A ratings)\n",
    "  - Round to 2 decimal place\n",
    "  Example: (8+8+7+6...)/(length of applicable dimensions)\n",
    "**ApplicableDimensions: The number of evaluation dimensions applied (typically 7)\n",
    "**PromptEffectiveness: \n",
    "  - Assign based on OverallScore:  \n",
    "    - \"Effective\" if OverallScore ≥ 7.5  \n",
    "    - \"Partially Effective\" if 4 ≤ OverallScore < 7.5  \n",
    "    - \"Ineffective\" if OverallScore < 4\n",
    "**Explanation: \n",
    "  - Concisely justify your ratings, referencing specific dimension scores,  \n",
    "  - Include a brief analysis of prompt strengths and weaknesses,  \n",
    "  - Note any relevant failure or success tags.\n",
    "\n",
    "--- \n",
    "\n",
    "Use the following dimensions to guide your evaluation:\n",
    "### 1. Purpose & Persona\n",
    "**Definition**: \n",
    "- “Purpose” clarifies why the prompt exists (e.g., to summarize, to translate, to analyze).\n",
    "- “Persona” defines for whom or from whose point of view the LLM should answer (e.g., “as a financial analyst,” “as a children’s book author”).\n",
    "**Key Indicators**:\n",
    "  - Does the User Prompt explicitly state its goal?\n",
    "  - Does it assign a clear persona/role to the model?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Both goal and persona are unambiguous.\n",
    "  (Example: “Summarize the following research paper as if you were a science journalist.”)\n",
    "  - Average(4-7): Either goal or persona is stated but one is vague.\n",
    "  (Example: “Write a summary. You’re a journalist.” No domain specified.)\n",
    "  - Bad(0-3): Neither purpose nor persona appears.\n",
    "  (Example: “Tell me about X.”)\n",
    "  - N/A: Explaining purpose/persona is unnecessary (e.g., a prompt that simply asks for a dictionary definition).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Requirements & Restrictions\n",
    "**Definition**: Explicit instructions about what must and must not be included in the answer (e.g., “Limit to 200 words,” “Do not mention sensitive data,” “Use bullet points only”).\n",
    "**Applicability**: If there are no constraints needed for a given task (e.g., “What is 2 + 2?”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Are there clear “must‐include” or “must‐avoid” guidelines?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicit, unambiguous constraints (e.g., “No pronouns; only third‐person narrative,” “Include three illustrative examples”).\n",
    "  - Average(4-7): Some constraints are present, but others are implied or incomplete (e.g., “Be concise” without a length target).\n",
    "  - Bad(0-3): No requirements or restrictions at all, leaving the model free to wander.\n",
    "  - N/A: The prompt’s nature makes constraints unnecessary (e.g., a simple “List the days of the week”).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Examples (Few‐Shot / Zero‐Shot)\n",
    "**Definition**: Whether the prompt provides explicit example inputs and outputs to guide the model (e.g., zero‐shot, one‐shot, or few‐shot formatting).\n",
    "**Applicability**: If the task does not benefit from example inputs/outputs (for instance, simple factual queries), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Are there sample question/answer pairs included (e.g., “Example: Q: … A: …”) that align with the intended task format?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Supplies clear, directly relevant examples that demonstrate exactly how to structure inputs and expected outputs.\n",
    "  - Average(4-7): Includes examples that are only partially aligned with the task or are too generic to serve as effective guidance.\n",
    "  - Bad(0-3): No examples are provided or requested, even though examples would significantly clarify format or expectations.\n",
    "  - N/A: Examples aren’t needed (e.g., a prompt asking, “What is the capital of France?”).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Context & Background\n",
    "**Definition**: Additional information about domain, audience, or relevant facts that the LLM must know to answer properly.\n",
    "**Applicability**: If the task requires no extra context (e.g., “Define photosynthesis”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt supply enough domain context?\n",
    "  (Example: “The following text is from a 19th‐century medical journal.”)\n",
    "  - Are audience considerations given?\n",
    "  (Example: “Explain this to a high‐school student.”)\n",
    "**Ratings**:\n",
    "  - Good(8-10): Detailed context and audience description aligned with the task.\n",
    "  - Average(4-7): Some context, but missing critical details, forcing assumptions.\n",
    "  - Bad(0-3): No context; model is left guessing domain or audience.\n",
    "  - N/A: Task is self‐contained and needs no additional context.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Instruction Structure\n",
    "**Definition**: The explicit format of instructions: a single directive, multi‐part numbered steps, or a choice (“answer in bullet points vs. essay”).\n",
    "**Applicability**: If structure is inherently trivial (e.g., “What is 5 × 7?”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Is the format clear (e.g., “Step 1: … Step 2: …”)?\n",
    "  - Does it specify whether the answer should be direct, stepwise, multi‐sectioned, etc.?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Well‐organized structure that matches the complexity (e.g., multi‐part instructions for multi‐stage tasks).\n",
    "  - Average(4-7):  Some structure but potentially inconsistent or too generic (“Answer in two parts”).\n",
    "  - Bad(0-3): No structural guidance—just a vague “Respond about X.”\n",
    "  - N/A: No structure needed because the task is extremely simple.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Style & Sensitivity\n",
    "**Definition**: Tone and register instructions (formal, friendly, technical), disclaimers (“I am not a lawyer”), and bias‐avoidance guidance.\n",
    "**Applicability**: If style or sensitivity is irrelevant (e.g., “Calculate the area of a circle with radius 3cm.”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt ask for a specific tone (“use clinical tone,” “avoid gender bias”)?\n",
    "  - Are disclaimers or sensitivity notices included when necessary?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicit style/tone and sensitivity cues.\n",
    "  - Average(4-7):  Partial style guidance (e.g., only “be professional” without elaboration).\n",
    "  - Bad(0-3):  No style or sensitivity guidance, even when sensitive content is expected.\n",
    "  - N/A: Style considerations are irrelevant for the given prompt.\n",
    "  \n",
    "---\n",
    "  \n",
    "### 7. Prompt Conciseness\n",
    "**Definition**: “Prompt Conciseness” refers to how efficiently the prompt communicates its intent, constraints, and expectations-using the fewest necessary words without sacrificing clarity or precision. A concise prompt avoids redundancy, filler words, and overly complex sentence structures while still being fully interpretable by the model. \n",
    "**Applicability**: If style or sensitivity is irrelevant (e.g., “Calculate the area of a circle with radius 3cm.”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt clearly convey all required instructions in a compact form?\n",
    "  - Are there unnecessary qualifiers, repetitions, or verbose phrasings?\n",
    "**Ratings**:\n",
    "  - Good(8-10): The prompt is compact and clearly communicates intent, requirements, and role without unnecessary elaboration.\n",
    "  - Average(4-7):  The prompt conveys the main idea but includes minor redundancies or could be made more direct without losing clarity. \n",
    "  - Bad(0-3):  The prompt is overly wordy, vague, or includes irrelevant information that obscures the main task. \n",
    "  - N/A: Prompt Conciseness are irrelevant for the given prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Evaluation Questions:\n",
    "\n",
    "1. Did the **USER PROMPT** satisfy all critical expectations and requirements posed by the **QUESTION**?\n",
    "2. If the prompt failed to meet expectations, was this due to the question itself being poorly constructed, ambiguous, or lacking proper scaffolding?\n",
    "3. Alternatively, if the prompt was shallow, incomplete, or ineffective despite the question being well-constructed, then the prompt is ineffective because its limitations were exposed.\n",
    "  \n",
    "---\n",
    "\n",
    "### Explanation Instructions:\n",
    "\n",
    "Provide a detailed explanation justifying the overall evaluation score (0 to 10):\n",
    "\n",
    "- Describe how the prompt performed against each applicable dimension.\n",
    "- Clearly highlight strengths and weaknesses.\n",
    "- Identify specific failure tags if the score is below 9, explaining why those failures occurred.\n",
    "- Reference all relevant dimensions and failure reasons explicitly.\n",
    "- For strong responses (score 9–10), emphasize the well-handled dimensions and why they were effective.\n",
    "\n",
    "This explanation should provide a clear rationale that helps prompt engineers and developers understand what worked, what didn’t, and how to improve.\n",
    "\n",
    "---\n",
    "\n",
    "### VERY IMPORTANT:\n",
    "- You must return the response **strictly in valid JSON**.\n",
    "- Do NOT use Markdown.\n",
    "- Do NOT include any preamble, footnotes, or commentary—just return the raw JSON object.\n",
    "\n",
    "\"example_output\": \n",
    "  {{\n",
    "    \"Purpose & Persona\": {{\n",
    "      \"Qualitative\": \"Good\",\n",
    "      \"Score\": 9\n",
    "    }},\n",
    "    \"Requirements & Restrictions\": {{\n",
    "      \"Qualitative\": \"N/A\",\n",
    "      \"Score\": null\n",
    "    }},\n",
    "    \"Examples (Few‐Shot / Zero‐Shot)\": {{\n",
    "      \"Qualitative\": \"Bad\",\n",
    "      \"Score\": 2\n",
    "    }},\n",
    "    \"Context & Background\": {{\n",
    "      \"Qualitative\": \"Average\",\n",
    "      \"Score\": 6\n",
    "    }},\n",
    "    \"Instruction Structure\": {{\n",
    "      \"Qualitative\": \"Good\",\n",
    "      \"Score\": 8\n",
    "    }},\n",
    "    \"Style & Sensitivity\": {{\n",
    "      \"Qualitative\": \"N/A\",\n",
    "      \"Score\": null\n",
    "    }},\n",
    "    \"Prompt Conciseness\": {{\n",
    "      \"Qualitative\": \"Average\",\n",
    "      \"Score\": 5\n",
    "    }},\n",
    "    \"ApplicableDimensions\": 5,\n",
    "    \"OverallScore\": 6.0,\n",
    "    \"PromptEffectiveness\": \"Partially Effective\",\n",
    "    \"Explanation\": \"Purpose & Persona: Clear role definition (9/10). Requirements: N/A per task nature. Examples: Lacks demonstration (2/10). Context: Adequate but verbose (6/10). Structure: Logical flow (8/10). Style: N/A. Conciseness: Contains redundant phrases (5/10). Major weakness: Absence of few-shot examples limits effectiveness.\"\n",
    "  }}\n",
    "  \n",
    "---\n",
    "\n",
    "### EXAMPLE RESPONSE FORMAT: Strictly Return only in json format\n",
    "\n",
    "{{\n",
    "    \"Purpose & Persona\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Requirements & Restrictions\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Examples (Few‐Shot / Zero‐Shot)\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Context & Background\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Instruction Structure\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Style & Sensitivity\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Prompt Conciseness\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"ApplicableDimensions\": <count of dimensions with non-null scores>,\n",
    "    \"OverallScore\": (float) <calculated average of all non-null or non-N/A scores (sum of non-null scores/length of ApplicableDimensions)>,\n",
    "    \"PromptEffectiveness\": \"Effective\" | \"Partially Effective\" | \"Ineffective\",\n",
    "    \"Explanation\": \"concise yet detailed explanation referencing dimension ratings, strengths, weaknesses, and failure tags\"\n",
    "  \n",
    "}}\n",
    "\n",
    "\n",
    "\n",
    "USER PROMPT:\n",
    "{user_prompt}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "      \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with a valid JSON object.\"},\n",
    "                {\"role\": \"user\", \"content\": prompteff}\n",
    "            ],\n",
    "            max_completion_tokens=512\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in judge_response]: {e}\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94b3496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(user_prompt, question, model, model_response):\n",
    "    \"\"\"\n",
    "    Evaluate the complexity and effectiveness of a user prompt and model response.\n",
    "\n",
    "    Parameters:\n",
    "    - user_prompt (str): The prompt provided by the user.\n",
    "    - question (str): The question or task to be evaluated.\n",
    "    - model (str): The model used to generate the response.\n",
    "    - model_response (str): The response generated by the model.\n",
    "\n",
    "    Returns:\n",
    "    - result_data (dict): A dictionary containing all evaluation metrics and parsed outputs.\n",
    "    \"\"\"\n",
    "    # Initialize result data with default values\n",
    "    result_data = {\n",
    "        \"score1\": None,\n",
    "        \"score2\": None,\n",
    "        \"net_score\": None,\n",
    "        \"challenging\": None,\n",
    "        \"effectiveness_score\": None,\n",
    "        \"effectiveness_text\": None,\n",
    "        \"raw_complexity_check\": \"ERROR\",\n",
    "        \"raw_complexity_eval\": \"ERROR\",\n",
    "        \"raw_effectiveness\": \"ERROR\"\n",
    "    }\n",
    "    \n",
    "    # STEP 1: Complexity Check using check_complexity_criteria\n",
    "    result = check_complexity_criteria(user_prompt, question, model)\n",
    "    try:\n",
    "        json_str = re.search(r\"\\{.*\\}\", result, re.DOTALL).group()\n",
    "        parsed_result = json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", result)\n",
    "        return result_data\n",
    "\n",
    "    print(\"\\n=== Complexity check Result ===\")\n",
    "    print(json.dumps(parsed_result, indent=4))\n",
    "    \n",
    "    # Extract Score1 and Dimensions\n",
    "    score1 = parsed_result.get(\"Score\")\n",
    "    if score1 is None:\n",
    "        score1 = 0\n",
    "    \n",
    "    dimensions = parsed_result.get(\"Dimensions in prompt\")\n",
    "    print(\"\\nScore1:\", score1)\n",
    "    print(\"Dimensions present in prompt:\", dimensions)\n",
    "    result_data[\"score1\"] = score1\n",
    "        \n",
    "    # STEP 2: Judge model response based on the number of dimensions\n",
    "    evaluation = judge_response(user_prompt, model_response, len(dimensions),model)\n",
    "    try:\n",
    "        json_str1 = re.search(r\"\\{.*\\}\", evaluation, re.DOTALL).group()\n",
    "        parsed_result1 = json.loads(json_str1)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", evaluation)\n",
    "        return result_data\n",
    "\n",
    "    print(\"\\n=== Complexity Evaluation Result ===\")\n",
    "    print(json.dumps(parsed_result1, indent=4))\n",
    "    \n",
    "    # Extract Score2\n",
    "    score2 = parsed_result1.get(\"OverallScore\")\n",
    "    if score2 is None:\n",
    "        score2 = 0\n",
    "\n",
    "    print(\"\\nScore2:\", score2)\n",
    "    result_data[\"score2\"] = score2\n",
    "    \n",
    "    # STEP 3: Compute Net Score and Challenging Status\n",
    "    if score1 is not None and score2 is not None:\n",
    "        net_score = (score1 + score2) / 2\n",
    "        print(\"\\nNet_score:\", net_score)\n",
    "        result_data[\"net_score\"] = net_score\n",
    "        result_data[\"challenging\"] = \"Yes\" if net_score > 0.5 else \"No\"\n",
    "        if net_score <= 0.5:\n",
    "            print(\"\\nPrompt is not challenging the model.\")\n",
    "        else:\n",
    "            print(\"\\nPrompt is challenging the model.\")\n",
    "    else:\n",
    "        print(\"\\nCannot compute net score: one or both scores are missing.\")\n",
    "        result_data[\"challenging\"] = \"Unknown\"\n",
    "      \n",
    "      \n",
    "    # STEP 4: Evaluate Prompt Effectiveness    \n",
    "    result2 = prompteffectiveness(user_prompt, question, model)\n",
    "    try:\n",
    "        json_str3 = re.search(r\"\\{.*\\}\", result2, re.DOTALL).group()\n",
    "        parsed_result3 = json.loads(json_str3)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", result2)\n",
    "        return result_data\n",
    "\n",
    "    # Print result summary\n",
    "    print(\"\\n=== Effectiveness check Result ===\")\n",
    "    print(json.dumps(parsed_result3, indent=4))  \n",
    "    \n",
    "    ans1 = parsed_result3.get(\"OverallScore\") \n",
    "    print(\"\\nPromptEffectivenessScore:\", ans1) \n",
    "    ans2 = parsed_result3.get(\"PromptEffectiveness\") \n",
    "    print(\"\\nPromptEffectiveness:\", ans2) \n",
    "    result_data[\"effectiveness_score\"] = ans1\n",
    "    result_data[\"effectiveness_text\"] = ans2\n",
    "    result_data[\"raw_complexity_check\"] = json.dumps(parsed_result, indent=4)\n",
    "    result_data[\"raw_complexity_eval\"] = json.dumps(parsed_result1, indent=4)\n",
    "    result_data[\"raw_effectiveness\"] = json.dumps(parsed_result3, indent=4)\n",
    "    return result_data\n",
    "    \n",
    "\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f0f09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Complexity check Result ===\n",
      "{\n",
      "    \"Dimensions in question\": [\n",
      "        \"Nested / Multi-Step Instructions\",\n",
      "        \"Inter-Dependent Constraints\",\n",
      "        \"Ambiguity Resolution\"\n",
      "    ],\n",
      "    \"TotalDimensions in question\": 3,\n",
      "    \"Dimensions in prompt\": [\n",
      "        \"Nested / Multi-Step Instructions\",\n",
      "        \"Inter-Dependent Constraints\",\n",
      "        \"Ambiguity Resolution\"\n",
      "    ],\n",
      "    \"TotalDimensions in prompt\": 3,\n",
      "    \"Score\": 1.0\n",
      "}\n",
      "\n",
      "Score1: 1.0\n",
      "Dimensions present in prompt: ['Nested / Multi-Step Instructions', 'Inter-Dependent Constraints', 'Ambiguity Resolution']\n",
      "\n",
      "=== Complexity Evaluation Result ===\n",
      "{\n",
      "    \"Nested / Multi\\u2010Step Instructions\": {\n",
      "        \"Qualitative\": \"Bad\",\n",
      "        \"Score\": 1.0\n",
      "    },\n",
      "    \"Inter\\u2010Dependent Constraints\": {\n",
      "        \"Qualitative\": \"Average\",\n",
      "        \"Score\": 0.6\n",
      "    },\n",
      "    \"Edge\\u2010Case Handling\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 0.3\n",
      "    },\n",
      "    \"ApplicableDimensions\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\",\n",
      "        \"Inter\\u2010Dependent Constraints\",\n",
      "        \"Edge\\u2010Case Handling\"\n",
      "    ],\n",
      "    \"normalizedScore\": 1.9,\n",
      "    \"OverallScore\": 0.63,\n",
      "    \"Explanation\": \"The model had significant issues with Nested / Multi-Step Instructions, failing to correctly choose between adding the 'Speech Pronunciation Coach' feature based on user analysis. This shows the prompt was challenging. The model handled Inter-Dependent Constraints moderately well, identifying the primary growth channel and defining a corresponding success metric, but misinterpreted some conditions. Edge-Case Handling was strong, accurately stating assumptions related to pricing, indicating this aspect of the prompt was less challenging.\",\n",
      "    \"FailureTags\": [\n",
      "        \"Omission\",\n",
      "        \"Misconditional\"\n",
      "    ],\n",
      "    \"StrengthAreas\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\"\n",
      "    ],\n",
      "    \"ImprovementAreas\": [\n",
      "        \"Edge\\u2010Case Handling\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Score2: 0.63\n",
      "\n",
      "Net_score: 0.815\n",
      "\n",
      "Prompt is challenging the model.\n",
      "\n",
      "=== Effectiveness check Result ===\n",
      "{\n",
      "    \"Purpose & Persona\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Requirements & Restrictions\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Examples (Few\\u2010Shot / Zero\\u2010Shot)\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Context & Background\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 8\n",
      "    },\n",
      "    \"Instruction Structure\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Style & Sensitivity\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Prompt Conciseness\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 8\n",
      "    },\n",
      "    \"ApplicableDimensions\": 5,\n",
      "    \"OverallScore\": 8.6,\n",
      "    \"PromptEffectiveness\": \"Effective\",\n",
      "    \"Explanation\": \"The prompt effectively communicates the purpose and persona, specifying the role of a product manager for crafting a PRD (9/10). Requirements are clear and detailed, outlining specific steps and constraints for the PRD creation (9/10). The context is well-defined, providing background on the task and audience (8/10). Instruction structure is logically mapped out with multi-step directives (9/10). Prompt conciseness is high, efficiently communicating complex instructions without unnecessary detail (8/10). Examples and style considerations are not applicable due to the structured nature of the prompt.\"\n",
      "}\n",
      "\n",
      "PromptEffectivenessScore: 8.6\n",
      "\n",
      "PromptEffectiveness: Effective\n",
      "\n",
      "=== Complexity check Result ===\n",
      "{\n",
      "    \"Dimensions in question\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\",\n",
      "        \"Inter\\u2010Dependent Constraints\",\n",
      "        \"Ambiguity Resolution\"\n",
      "    ],\n",
      "    \"TotalDimensions in question\": 3,\n",
      "    \"Dimensions in prompt\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\",\n",
      "        \"Inter\\u2010Dependent Constraints\"\n",
      "    ],\n",
      "    \"TotalDimensions in prompt\": 2,\n",
      "    \"Score\": 0.67\n",
      "}\n",
      "\n",
      "Score1: 0.67\n",
      "Dimensions present in prompt: ['Nested / Multi‐Step Instructions', 'Inter‐Dependent Constraints']\n",
      "\n",
      "=== Complexity Evaluation Result ===\n",
      "{\n",
      "    \"Nested / Multi\\u2010Step Instructions\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 0.3\n",
      "    },\n",
      "    \"Inter\\u2010Dependent Constraints\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 0.3\n",
      "    },\n",
      "    \"ApplicableDimensions\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\",\n",
      "        \"Inter\\u2010Dependent Constraints\"\n",
      "    ],\n",
      "    \"normalizedScore\": 0.6,\n",
      "    \"OverallScore\": 0.3,\n",
      "    \"Explanation\": \"The model effectively handled the prompt, following the nested, multi-step instructions accurately and addressing the inter-dependent constraints correctly. It included the 'Speech Pronunciation Coach' under beginners when it identified that most users were beginners, and used 'viral referral rate' since social media was chosen as the main channel. This success indicates that the prompt was not particularly challenging for the model in these aspects.\",\n",
      "    \"FailureTags\": [],\n",
      "    \"StrengthAreas\": [],\n",
      "    \"ImprovementAreas\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\",\n",
      "        \"Inter\\u2010Dependent Constraints\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Score2: 0.3\n",
      "\n",
      "Net_score: 0.485\n",
      "\n",
      "Prompt is not challenging the model.\n",
      "\n",
      "=== Effectiveness check Result ===\n",
      "{\n",
      "    \"Purpose & Persona\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Requirements & Restrictions\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Examples (Few\\u2010Shot / Zero\\u2010Shot)\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Context & Background\": {\n",
      "        \"Qualitative\": \"Average\",\n",
      "        \"Score\": 6\n",
      "    },\n",
      "    \"Instruction Structure\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 10\n",
      "    },\n",
      "    \"Style & Sensitivity\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Prompt Conciseness\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 8\n",
      "    },\n",
      "    \"ApplicableDimensions\": 5,\n",
      "    \"OverallScore\": 8.4,\n",
      "    \"PromptEffectiveness\": \"Effective\",\n",
      "    \"Explanation\": \"Purpose & Persona: The prompt clearly defines the goal of generating a PRD and the roles and perspectives involved (9/10). Requirements & Restrictions: Explicit instructions give detailed constraints for each step (9/10). Examples: N/A as the type of task doesn't require them. Context: Provides some context but lacks deeper domain-specific background (6/10). Instruction Structure: Excellent organization, clear multi-step instructions (10/10). Style and Sensitivity: N/A for this prompt. Conciseness: Efficient, though could omit or rephrase minor redundancies (8/10). No major weaknesses, overall a well-structured and clear prompt suited for complex task demands.\"\n",
      "}\n",
      "\n",
      "PromptEffectivenessScore: 8.4\n",
      "\n",
      "PromptEffectiveness: Effective\n",
      "\n",
      "=== Complexity check Result ===\n",
      "{\n",
      "    \"Dimensions in question\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\",\n",
      "        \"Inter\\u2010Dependent Constraints\",\n",
      "        \"Ambiguity / Vagueness Handling\"\n",
      "    ],\n",
      "    \"TotalDimensions in question\": 3,\n",
      "    \"Dimensions in prompt\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\"\n",
      "    ],\n",
      "    \"TotalDimensions in prompt\": 1,\n",
      "    \"Score\": 0.33\n",
      "}\n",
      "\n",
      "Score1: 0.33\n",
      "Dimensions present in prompt: ['Nested / Multi‐Step Instructions']\n",
      "\n",
      "=== Complexity Evaluation Result ===\n",
      "{\n",
      "    \"Nested / Multi\\u2010Step Instructions\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 0.3\n",
      "    },\n",
      "    \"Conflicting Instructions\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Inter\\u2010Dependent Constraints\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Edge\\u2010Case Handling\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Ambiguity Resolution\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Domain Fusion\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Multi\\u2010Source/Modal Analysis\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Hypothetical / Counterfactual Reasoning\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"ApplicableDimensions\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\"\n",
      "    ],\n",
      "    \"normalizedScore\": 0.3,\n",
      "    \"OverallScore\": 0.3,\n",
      "    \"Explanation\": \"The model correctly followed all steps required by the prompt. It included the main sections: Overview, Key Features, and Target Users, each with the necessary details. There were no omissions in the steps, and the execution order of the instructions was correct. This indicates that the prompt did not significantly challenge the model, due to its good performance.\",\n",
      "    \"FailureTags\": [],\n",
      "    \"StrengthAreas\": [],\n",
      "    \"ImprovementAreas\": [\n",
      "        \"Nested / Multi\\u2010Step Instructions\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Score2: 0.3\n",
      "\n",
      "Net_score: 0.315\n",
      "\n",
      "Prompt is not challenging the model.\n",
      "\n",
      "=== Effectiveness check Result ===\n",
      "{\n",
      "    \"Purpose & Persona\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Requirements & Restrictions\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 8\n",
      "    },\n",
      "    \"Examples (Few\\u2010Shot / Zero\\u2010Shot)\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Context & Background\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 8\n",
      "    },\n",
      "    \"Instruction Structure\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 9\n",
      "    },\n",
      "    \"Style & Sensitivity\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Prompt Conciseness\": {\n",
      "        \"Qualitative\": \"Good\",\n",
      "        \"Score\": 8\n",
      "    },\n",
      "    \"ApplicableDimensions\": 5,\n",
      "    \"OverallScore\": 8.4,\n",
      "    \"PromptEffectiveness\": \"Effective\",\n",
      "    \"Explanation\": \"Purpose & Persona: Clearly defines role and task as a senior product manager crafting a PRD for a new app (9/10). Requirements: Specific guidelines including word limit, feature count, and format requirements ensure clarity (8/10). Examples: Not necessary given the explicit nature of the task. Context: Provides adequate background, identifying target market as urban professionals (8/10). Instruction Structure: Well-organized, multi-sectioned with specific guidance (9/10). Style: Not applicable. Conciseness: Clear and direct, effectively communicates task without unnecessary detail (8/10).\"\n",
      "}\n",
      "\n",
      "PromptEffectivenessScore: 8.4\n",
      "\n",
      "PromptEffectiveness: Effective\n",
      "\n",
      "=== Complexity check Result ===\n",
      "{\n",
      "    \"Dimensions in question\": [\n",
      "        \"Nested / Multi-Step Instructions\",\n",
      "        \"Inter-Dependent Constraints\",\n",
      "        \"Ambiguity Resolution\"\n",
      "    ],\n",
      "    \"TotalDimensions in question\": 3,\n",
      "    \"Dimensions in prompt\": [],\n",
      "    \"TotalDimensions in prompt\": 0,\n",
      "    \"Score\": 0.0\n",
      "}\n",
      "\n",
      "Score1: 0.0\n",
      "Dimensions present in prompt: []\n",
      "\n",
      "=== Complexity Evaluation Result ===\n",
      "{\n",
      "    \"Nested / Multi\\u2010Step Instructions\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Conflicting Instructions\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Inter\\u2010Dependent Constraints\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Edge\\u2010Case Handling\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Ambiguity Resolution\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Domain Fusion\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Multi\\u2010Source/Modal Analysis\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Hypothetical / Counterfactual Reasoning\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"ApplicableDimensions\": [],\n",
      "    \"normalizedScore\": 0.0,\n",
      "    \"OverallScore\": 0.0,\n",
      "    \"Explanation\": \"Since no specific cognitive and reasoning dimensions were requested by the prompt, there are no applicable dimensions to evaluate the performance of the model's response. The user's prompt required a straightforward task of writing a product requirements document without additional complexity.\",\n",
      "    \"FailureTags\": [],\n",
      "    \"StrengthAreas\": [],\n",
      "    \"ImprovementAreas\": []\n",
      "}\n",
      "\n",
      "Score2: 0.0\n",
      "\n",
      "Net_score: 0.0\n",
      "\n",
      "Prompt is not challenging the model.\n",
      "\n",
      "=== Effectiveness check Result ===\n",
      "{\n",
      "    \"Purpose & Persona\": {\n",
      "        \"Qualitative\": \"Bad\",\n",
      "        \"Score\": 2\n",
      "    },\n",
      "    \"Requirements & Restrictions\": {\n",
      "        \"Qualitative\": \"Bad\",\n",
      "        \"Score\": 3\n",
      "    },\n",
      "    \"Examples (Few\\u2010Shot / Zero\\u2010Shot)\": {\n",
      "        \"Qualitative\": \"Bad\",\n",
      "        \"Score\": 1\n",
      "    },\n",
      "    \"Context & Background\": {\n",
      "        \"Qualitative\": \"Bad\",\n",
      "        \"Score\": 2\n",
      "    },\n",
      "    \"Instruction Structure\": {\n",
      "        \"Qualitative\": \"Bad\",\n",
      "        \"Score\": 3\n",
      "    },\n",
      "    \"Style & Sensitivity\": {\n",
      "        \"Qualitative\": \"N/A\",\n",
      "        \"Score\": null\n",
      "    },\n",
      "    \"Prompt Conciseness\": {\n",
      "        \"Qualitative\": \"Average\",\n",
      "        \"Score\": 5\n",
      "    },\n",
      "    \"ApplicableDimensions\": 6,\n",
      "    \"OverallScore\": 2.67,\n",
      "    \"PromptEffectiveness\": \"Ineffective\",\n",
      "    \"Explanation\": \"The prompt fails to specify any persona or purpose (Purpose & Persona: 2/10), lacking clarity on who should craft the PRD and the goal. Requirements & Restrictions are weak as no clear constraints are provided, such as content to include in the PRD (3/10). There are no examples to guide format or expectations, hindering effectiveness significantly (1/10). Context & Background are also lacking, with no mention of audience or specific needs of the app (2/10). Instruction Structure is vague with no indication of the required format or order of sections in the PRD (3/10). While concise, the prompt misses critical elements for task completion (Conciseness: 5/10). Major failure: absence of detailed purpose, structure, and requirements.\"\n",
      "}\n",
      "\n",
      "PromptEffectivenessScore: 2.67\n",
      "\n",
      "PromptEffectiveness: Ineffective\n",
      "Evaluation results saved to sample_output_openai.csv\n"
     ]
    }
   ],
   "source": [
    "# Load CSV and process each row\n",
    "def process_csv(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Processes a CSV file by reading user prompts and questions,\n",
    "    generating responses using MODEL_1, and evaluating them using MODEL_2.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file: Path to the input CSV file.\n",
    "    - output_file: Path to save the output CSV file with results.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(input_file, encoding='utf-8', encoding_errors='replace')\n",
    "    output_rows = [] # Store output data for each row\n",
    "    \n",
    "    # Iterate over each row in the CSV\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        user_prompt = row['user_prompt']\n",
    "\n",
    "        try:\n",
    "            # Generate model response\n",
    "            model_response = generate_response(user_prompt, MODEL_1)\n",
    "            # Evaluate the response\n",
    "            result = evaluate(user_prompt, question, MODEL_2, model_response)\n",
    "        except Exception as e:\n",
    "            # Handle any unexpected errors gracefully and log them\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            # Store default error values in case of exception\n",
    "            result = {\n",
    "                \"score1\": None,\n",
    "                \"score2\": None,\n",
    "                \"net_score\": None,\n",
    "                \"challenging\": None,\n",
    "                \"effectiveness_score\": None,\n",
    "                \"effectiveness_text\": None,\n",
    "                \"raw_complexity_check\": \"ERROR\",\n",
    "                \"raw_complexity_eval\": \"ERROR\",\n",
    "                \"raw_effectiveness\": \"ERROR\"\n",
    "            }\n",
    "        \n",
    "        # Append the results to the output list\n",
    "        output_rows.append({\n",
    "            \"question\": question,\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"score1\": result[\"score1\"],\n",
    "            \"score2\": result[\"score2\"],\n",
    "            \"net_score\": result[\"net_score\"],\n",
    "            \"prompt_challenging\": result[\"challenging\"],\n",
    "            \"effectiveness_score\": result[\"effectiveness_score\"],\n",
    "            \"effectiveness_text\": result[\"effectiveness_text\"],\n",
    "            \"complexity_check_json\": result.get(\"raw_complexity_check\", \"\"),\n",
    "            \"complexity_eval_json\": result.get(\"raw_complexity_eval\", \"\"),\n",
    "            \"effectiveness_json\": result.get(\"raw_effectiveness\", \"\")   \n",
    "        })\n",
    "\n",
    "    # Convert the list of result dictionaries into a DataFrame\n",
    "    out_df = pd.DataFrame(output_rows)\n",
    "    \n",
    "    # Save the results to a new CSV file\n",
    "    out_df.to_csv(output_file, index=False)\n",
    "    print(f\"Evaluation results saved to {output_file}\")\n",
    "\n",
    "\n",
    "# ====== USAGE EXAMPLE ======\n",
    "# Input and output file paths\n",
    "input_csv_path = \"sample_input.csv\"\n",
    "output_csv_path = \"sample_output_openai.csv\"\n",
    "\n",
    "# Run the processing function\n",
    "process_csv(input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176f483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebd304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
