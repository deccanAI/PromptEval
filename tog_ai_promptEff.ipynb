{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e33372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found? True\n",
      "API Key (masked): tgp_v...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "print(f\"API key found? {bool(api_key)}\")\n",
    "print(f\"API Key (masked): {api_key[:5]}...\") \n",
    "\n",
    "import together\n",
    "together.api_key = api_key\n",
    "client = together.Together()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eabc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MODEL_2 = \"Qwen/Qwen2.5-7B-Instruct-Turbo\"\n",
    "# MODEL_1 = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "MODEL_1 = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "MODEL_2 = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# MODEL_1 = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# MODEL_1 = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n",
    "# MODEL_1 = \"google/gemma-2-27b-it\"\n",
    "# MODEL_2 = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4fbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model response\n",
    "# def generate_response(prompt, model):\n",
    "#     try:\n",
    "#         response = together.Complete.create(\n",
    "#             prompt=prompt,\n",
    "#             model=model,\n",
    "#             max_tokens=512,\n",
    "#             temperature=0.3\n",
    "#         )\n",
    "#         return response['choices'][0]['text'].strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"[Error in generate_response]: {e}\"\n",
    "\n",
    "def generate_response(prompt: str, model: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in generate_response]: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d7063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_complexity_criteria(user_prompt: str, question: str, model):\n",
    "    complexity_prompt = f\"\"\"\n",
    "You are an expert evaluator. Your job is to verify whether the complexity dimensions mentioned in the QUESTION are clearly implemented in the USER PROMPT.\n",
    "\n",
    "### DIMENSIONS CHECKLIST :\n",
    "\n",
    "### 1. Nested / Multi‐Step Instructions\n",
    "**Definition**: The prompt requires the model to perform multiple subtasks in a specified order (e.g., “Step 1: …, Step 2: …, Step 3: …”).\n",
    "**Key Indicators**:\n",
    "  - Are there explicit numbered or indented steps?\n",
    "  - Does one instruction logically precede another?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Conflicting Instructions\n",
    "**Definition**: The prompt imposes two or more constraints that cannot all be satisfied simultaneously (e.g., “Write a summary in under 50 words but include five examples”).\n",
    "**Key Indicators**:\n",
    "  - Do two (or more) requirements directly contradict each other?\n",
    "  - Is the prompt forcing the model to choose or partially satisfy conflicting directives?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inter‐Dependent Constraints\n",
    "**Definition**: One instruction becomes active only if another condition is met (e.g., “If the text contains more than ten technical terms, define each; otherwise, just list them”).\n",
    "**Key Indicators**:\n",
    "  - Is there an explicit “if‐then” or “only when” dependency?\n",
    "  - Does satisfying the second instruction depend on the first instruction’s outcome?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Edge‐Case Handling\n",
    "**Definition**: The prompt instructs the model to explicitly admit when required information is missing or when an edge case arises (e.g., “If the input lacks dates, respond ‘Insufficient data’”).\n",
    "**Key Indicators**:\n",
    "  - Does the prompt say “If X is absent or unclear, do not guess”?\n",
    "  - Are instructions provided for how to behave if required data is missing?\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ambiguity Resolution\n",
    "**Definition**: The prompt introduces ambiguous terms or references and instructs the model to clarify or handle them (e.g., “The word ‘bank’ could mean financial institution or riverbank—state both interpretations”).\n",
    "**Key Indicators**:\n",
    "  - Are ambiguous words or phrases flagged?\n",
    "  - Does the prompt ask “If ambiguous, explain interpretations”?\n",
    "---\n",
    "\n",
    "### 6. Domain Fusion\n",
    "**Definition**: The prompt fuses two or more specialized domains into one task (e.g., “Analyze the legal contract’s economic impact using statistical models”).\n",
    "**Key Indicators**:\n",
    "  - Are at least two distinct fields explicitly mentioned?\n",
    "  - Does the prompt require coherent integration of knowledge from both domains?\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Multi‐Source/Modal Analysis\n",
    "**Definition**: The model must reason over multiple distinct inputs (e.g., two text passages, text plus described visuals) and synthesize across them.\n",
    "**Key Indicators**:\n",
    "  - Are there at least two distinct “sources” described?\n",
    "  - Does the prompt instruct the model to integrate information across those sources?\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Hypothetical / Counterfactual Reasoning\n",
    "**Definition**: The prompt poses a scenario contrary to known facts or purely hypothetical (e.g., “If gravity were inverted, describe consequences for river flow”).\n",
    "**Key Indicators**:\n",
    "  - Does it present an “as if” scenario explicitly defying reality?\n",
    "  - Are you instructed to treat that scenario as true and reason within it?\n",
    "\n",
    "---\n",
    "Be accurate, thorough, and follow instructions step-by-step.\n",
    "\n",
    "You MUST follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### STEP 1: IDENTIFY DIMENSIONS IN QUESTION\n",
    "\n",
    "From the QUESTION text below, identify all the complexity dimensions it expects the prompt to include.\n",
    "Only consider a dimension to be expected if:\n",
    "- Its core definition or indicators are described or implied.\n",
    "\n",
    "---\n",
    "\n",
    "### STEP 2: VERIFY DIMENSIONS IN USER PROMPT\n",
    "\n",
    "Next, check which of the above dimensions are **actually present** in the USER PROMPT.\n",
    "\n",
    "Only mark a dimension as present if:\n",
    "- The USER PROMPT shows clear evidence of fulfilling the definition or key indicators of that dimension.\n",
    "- You can justify its presence using the specific examples listed in the DIMENSION CHECKLIST.\n",
    "\n",
    "---\n",
    "\n",
    "### STEP 3: SCORE AND EVALUATE\n",
    "\n",
    "Use this formula to compute the score:\n",
    "\n",
    "    Score = (number of dimensions present in the USER PROMPT) / (number of dimensions required by the QUESTION)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Very Important and give more weightage -->>***Please return your answer *strictly in JSON* using the format below — no markdown, no extra explanation:***\n",
    "### VERY IMPORTANT:\n",
    "- You must return the response **strictly in valid JSON**.\n",
    "- Do NOT use Markdown.\n",
    "- Do NOT include any preamble, footnotes, or commentary—just return the raw JSON object.\n",
    "\n",
    "### STEP 4: OUTPUT STRICTLY IN THIS JSON FORMAT:\n",
    "{{\n",
    "  \"Dimensions in question\": [\"Hypothetical / Counterfactual Reasoning\", \"Ambiguity\", \"Multi‐Source/Modal Analysis\", \"Domain Fusion\", \"Edge‐Case Handling\", ...],\n",
    "  \"TotalDimensions in question\": <count>,\n",
    "  \"Dimensions in prompt\": [\"Ambiguity\", \"Multi‐Source/Modal Analysis\", \"Domain Fusion\"...],\n",
    "  \"TotalDimensions in prompt\": <count>,\n",
    "  \"Score\": <float between 0 and 1, rounded to 2 decimals...(TotalDimensions in prompt)/(TotalDimensions in question)>\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "### INPUTS:\n",
    "- USER PROMPT: \"{user_prompt}\"\n",
    "- QUESTION: \"{question}\"\n",
    "\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = together.Complete.create(\n",
    "            prompt=complexity_prompt,\n",
    "            model=model,\n",
    "            max_tokens=512,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in complexity check]: {e}\"\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f00accc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function using LLM as judge\n",
    "def judge_response(user_prompt, model_response, total_dimensions):\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert language model evaluator.\n",
    "\n",
    "Your task is to assess how effectively a USER PROMPT challenged a MODEL, based on its RESPONSE. You will do this by analyzing the model’s performance across eight cognitive and reasoning dimensions.\n",
    "The more the model fails, the more effective and challenging the prompt is considered.\n",
    "\n",
    "Then, provide a **Summary** including:\n",
    "**ApplicableDimensions**: List[dimensions with non-null scores]\n",
    "**normalizedScore**: <sum of normalized scores>,\n",
    "**OverallScore**: <normalizedScore / D>,\n",
    "**Explanation: \"Summarize the model's performance across dimensions, highlighting where it failed and how that shows the prompt was strong.\",\n",
    "**FailureTags (if applicable): [\"Omission\", \"Ambiguity\", \"Conflict\", \"Shallow Reasoning\", ...],\n",
    "\n",
    "--- \n",
    "\n",
    "Use the following dimensions to guide your evaluation:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Nested / Multi‐Step Instructions\n",
    "**Definition**: The prompt requires the model to perform multiple subtasks in a specified order (e.g., “Step 1: …, Step 2: …, Step 3: …”).\n",
    "**Key Indicators**:\n",
    "  - Are there explicit numbered or indented steps?\n",
    "  - Does one instruction logically precede another?\n",
    "**Prompt Ratings** (based on model performance):\n",
    "  - Bad (8–10): The model failed — skipped steps, did them out of order, or gave shallow detail. Prompt is strong.\n",
    "  - Average (4–7): The model made small mistakes — minor ordering errors or partial steps. Prompt is moderately challenging.\n",
    "  - Good (0–3): The model followed all steps correctly. Prompt did not challenge the model.\n",
    "  - N/A: The prompt does not require multiple subtasks (e.g., a straightforward factual query).\n",
    "---\n",
    "\n",
    "### 2. Conflicting Instructions\n",
    "**Definition**: The prompt imposes two or more constraints that cannot all be satisfied simultaneously (e.g., “Write a summary in under 50 words but include five examples”).\n",
    "**Key Indicators**:\n",
    "  - Do two (or more) requirements directly contradict each other?\n",
    "  - Is the prompt forcing the model to choose or partially satisfy conflicting directives?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model failed to resolve the conflict, picked blindly, or gave up. Prompt exposed limitations.\n",
    "  - Average (4–7): The model noticed the conflict but handled it only partially or with weak justification. Prompt had mild challenge.\n",
    "  - Good (0–3): The model resolved the conflict gracefully and explained the trade-off. Prompt was too easy.\n",
    "  - N/A: The prompt has no conflicting requirements (e.g., simply “List all prime numbers under 100”).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inter‐Dependent Constraints\n",
    "**Definition**: One instruction becomes active only if another condition is met (e.g., “If the text contains more than ten technical terms, define each; otherwise, just list them”).\n",
    "**Key Indicators**:\n",
    "  - Is there an explicit “if‐then” or “only when” dependency?\n",
    "  - Does satisfying the second instruction depend on the first instruction’s outcome?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model ignored the conditional logic or applied it incorrectly. Prompt was logically challenging.\n",
    "  - Average (4–7): The model partially followed the conditions but missed nuances. Prompt was somewhat effective.\n",
    "  - Good (0–3): The model applied conditions precisely. Prompt failed to test dependency logic.\n",
    "  - N/A: No conditional instructions present (e.g., “Translate this sentence into Spanish”).\n",
    "---\n",
    "\n",
    "### 4. Edge‐Case Handling\n",
    "**Definition**: The prompt instructs the model to explicitly admit when required information is missing or when an edge case arises (e.g., “If the input lacks dates, respond ‘Insufficient data’”).\n",
    "**Key Indicators**:\n",
    "  - Does the prompt say “If X is absent or unclear, do not guess”?\n",
    "  - Are instructions provided for how to behave if required data is missing?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model hallucinated, guessed incorrectly, or failed to handle edge cases. Prompt forced the model to struggle.\n",
    "  - Average (4–7): The model sometimes handled edge cases, but not consistently. Prompt added some challenge.\n",
    "  - Good (0–3): The model correctly followed instructions for all missing or unclear input. Prompt lacked edge-case difficulty.\n",
    "  - N/A: The prompt’s context guarantees all information is present (e.g., “Compute the sum of the following five numbers”).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ambiguity Resolution\n",
    "**Definition**: The prompt introduces ambiguous terms or references and instructs the model to clarify or handle them (e.g., “The word ‘bank’ could mean financial institution or riverbank—state both interpretations”).\n",
    "**Key Indicators**:\n",
    "  - Are ambiguous words or phrases flagged?\n",
    "  - Does the prompt ask “If ambiguous, explain interpretations”?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model picked a single meaning without explanation or failed to identify the ambiguity. Prompt successfully revealed limits.\n",
    "  - Average (4–7): The model noticed ambiguity but didn’t explain well or fully. Prompt was moderately effective.\n",
    "  - Good (0–3): The model clearly explained all interpretations and justified its choice. Prompt wasn’t challenging.\n",
    "  - N/A: No ambiguity in the prompt (e.g., “List the first ten Fibonacci numbers”).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Domain Fusion\n",
    "**Definition**: The prompt fuses two or more specialized domains into one task (e.g., “Analyze the legal contract’s economic impact using statistical models”).\n",
    "**Key Indicators**:\n",
    "  - Are at least two distinct fields explicitly mentioned?\n",
    "  - Does the prompt require coherent integration of knowledge from both domains?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model failed to link domains, missed one entirely, or showed shallow understanding. Prompt was deeply integrative.\n",
    "  - Average (4–7): Model partially integrated domains or showed uneven depth. Prompt was somewhat integrative.\n",
    "  - Good (0–3): Model combined domains effortlessly and with accuracy. Prompt wasn’t cognitively complex.\n",
    "  - N/A: Task focuses on a single domain (e.g., “Explain basic thermodynamics”).\n",
    "---\n",
    "\n",
    "### 7. Multi‐Source/Modal Analysis\n",
    "**Definition**: The model must reason over multiple distinct inputs (e.g., two text passages, text plus described visuals) and synthesize across them.\n",
    "**Key Indicators**:\n",
    "  - Are there at least two distinct “sources” described?\n",
    "  - Does the prompt instruct the model to integrate information across those sources?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model ignored one or more sources or failed to synthesize. Prompt challenged multi-input reasoning.\n",
    "  - Average (4–7): The model mentioned multiple sources but lacked coherent synthesis. Prompt was modestly challenging.\n",
    "  - Good (0–3): The model used all sources well and produced a coherent result. Prompt failed to create input complexity.\n",
    "  - N/A: Task provides a single input (e.g., “Summarize this paragraph”).\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Hypothetical / Counterfactual Reasoning\n",
    "**Definition**: The prompt poses a scenario contrary to known facts or purely hypothetical (e.g., “If gravity were inverted, describe consequences for river flow”).\n",
    "**Key Indicators**:\n",
    "  - Does it present an “as if” scenario explicitly defying reality?\n",
    "  - Are you instructed to treat that scenario as true and reason within it?\n",
    "**Prompt Ratings**:\n",
    "  - Bad (8–10): The model reverted to real-world facts, ignored the hypothetical, or answered shallowly. Prompt tested imaginative reasoning.\n",
    "  - Average (4–7): The model stayed in the scenario but didn’t explore implications fully. Prompt had partial depth.\n",
    "  - Good (0–3): The model reasoned thoroughly and stayed within the hypothetical frame. Prompt was not mentally taxing.\n",
    "  - N/A: No hypothetical scenario—task uses real‐world facts only.\n",
    "\n",
    "---\n",
    "\n",
    "Evaluate the prompt and the model response using these questions:\n",
    "\n",
    "1. Did the model satisfy all critical expectations of the prompt?\n",
    "2. If the model failed, was it because the prompt was poorly constructed, overly ambiguous, or lacked proper scaffolding?\n",
    "3. Alternatively, if the model output was shallow or incorrect despite a strong prompt, then the prompt was effective because it exposed model limitations.\n",
    "4. Does It Break Nested / Multi-Step Instructions?\n",
    "  Failure Modes:\n",
    "  - Omission: The model skips one or more required steps.\n",
    "  - Wrong Order: Completes steps out of sequence.\n",
    "  - Incomplete Detail: Does not give enough detail for a subtask labeled “explain in detail.”\n",
    "\n",
    "5. Does It Break Conflicting Instructions?\n",
    "  Failure Modes:\n",
    "  - Blind Obedience: Tries to satisfy both contradictory instructions fully (and thus fails both).\n",
    "  - Undisclosed Choice: Picks one constraint without acknowledging the conflict.\n",
    "  - Refusal Without Explanation: Says “I can’t” but does not explain why.\n",
    "\n",
    "6. Does It Break Inter-Dependent Constraints?\n",
    "  Failure Modes:\n",
    "  - Misconditional: Applies the second constraint unconditionally or fails to check the first.\n",
    "  - Partial Compliance: Applies conditions incorrectly.\n",
    "\n",
    "7. Does It Break Edge-Case Handling?\n",
    "  Failure Modes:\n",
    "  - Fabrication: Hallucinates missing data instead of admitting it’s missing.\n",
    "  - Incorrect Catch: Says “insufficient data” when data actually exists.\n",
    "\n",
    "8. Does It Break Ambiguity Resolution?\n",
    "  Failure Modes:\n",
    "  - Single Interpretation: Chooses one meaning without acknowledging alternatives.\n",
    "  - Ignores Request: Fails to clarify ambiguous term.\n",
    "\n",
    "9. Does It Break Domain Fusion?\n",
    "  Failure Modes:\n",
    "  - Domain Drop: Only addresses one domain, ignoring the other.\n",
    "  - Shallow Fusion: Mentions the second domain only superficially.\n",
    "\n",
    "10. Does It Break Multi-Source/Modal Analysis?\n",
    "  Failure Modes:\n",
    "  - Single-Source Focus: Uses only one input, ignoring others.\n",
    "  - Surface Integration: Mentions multiple inputs but does not synthesize.\n",
    "\n",
    "11. Does It Break Hypothetical / Counterfactual Reasoning?\n",
    "  Failure Modes:\n",
    "  - Reality Check: Refuses or reaffirms real-world facts instead of engaging in the scenario.\n",
    "  - Shallow Answer: Acknowledges hypothetical but does not explore implications.\n",
    "  \n",
    "---\n",
    "\n",
    "Generate Explanation of Evaluation (0–10) with Failure Tags and Dimension-Based Analysis\n",
    "You are an evaluator assessing how well a model handled a complex prompt. Based on the score (from 0 to 10), write a detailed explanation that:\n",
    "1. Explains Why the Response Got This Score\n",
    "  - Describe how the model handled or failed each applicable complexity dimension.\n",
    "  - Clearly justify where the response was strong, and where it broke—i.e., failed to satisfy intended challenges.\n",
    "  - If the score is below 9, mention specific failure tags and explain why they apply.\n",
    "\n",
    "2. Use These Dimensions and Failure Modes\n",
    "Below are the 8 core dimensions you must consider. For each one, if it broke, use the relevant failure mode(s) and incorporate the corresponding failure tag(s) in your explanation.\n",
    "\n",
    "2.1. Nested / Multi-Step Instructions\n",
    "Failure Modes:\n",
    "  - Omission: Skips required step(s)\n",
    "  - Wrong Order: Steps executed out of sequence\n",
    "  - Incomplete Detail: Step lacks required elaboration\n",
    "Failure Tag: Missed nested instruction\n",
    "\n",
    "2.2. Conflicting Instructions\n",
    "Failure Modes:\n",
    "  - Blind Obedience: Tries to satisfy both conflicts fully\n",
    "  - Undisclosed Choice: Picks one side silently\n",
    "  - Refusal Without Explanation: Says “can’t” with no reason\n",
    "Failure Tag: Conflict evasion\n",
    "\n",
    "\n",
    "2.3. Inter-Dependent Constraints\n",
    "Failure Modes:\n",
    "  - Misconditional: Fails to apply logic in correct order\n",
    "  - Partial Compliance: Misinterprets conditions\n",
    "Failure Tag: Conditional failure\n",
    "\n",
    "2.4. Edge-Case Handling\n",
    "Failure Modes:\n",
    "  - Fabrication: Hallucinates data instead of admitting absence\n",
    "  - Incorrect Catch: Says data is missing when it's present\n",
    "Failure Tag: Edge case miss\n",
    "\n",
    "2.5. Ambiguity Resolution\n",
    "Failure Modes:\n",
    "  - Single Interpretation: Picks one meaning without context\n",
    "  - Ignores Request: Fails to clarify ambiguous terms\n",
    "Failure Tag: Ambiguity unresolved\n",
    "\n",
    "2.6. Domain Fusion\n",
    "Failure Modes:\n",
    "  - Domain Drop: Ignores one domain entirely\n",
    "  - Shallow Fusion: Surface-level integration\n",
    "Failure Tag: Domain omission\n",
    "\n",
    "2.7. Multi-Source / Modal Analysis\n",
    "Failure Modes:\n",
    "  - Single-Source Focus: Ignores other inputs\n",
    "  - Surface Integration: Uses but doesn't connect sources\n",
    "Failure Tag: Source integration failure\n",
    "\n",
    "2.8. Hypothetical / Counterfactual Reasoning\n",
    "Failure Modes:\n",
    "  - Reality Check: Rejects scenario, defaults to facts\n",
    "  - Shallow Answer: Ignoled implication depth\n",
    "Failure Tag: Shallow hypothetical\n",
    "\n",
    "3. Format Your Explanation Like This\n",
    "  - Reference all relevant dimensions and failure tags.\n",
    "  - If the response is strong (score 9–10), describe which dimensions were handled well and why.\n",
    "Goal\n",
    "The explanation should provide a clear rationale for the evaluation score, tied to concrete dimension-based reasoning. It must help prompt engineers or model developers understand what failed, why it failed, and how it can improve.\n",
    "\n",
    "---\n",
    "\n",
    "##  SCORING ONLY ON APPLICABLE DIMENSIONS\n",
    "\n",
    "You must only evaluate and score those dimensions which are listed in the input as **Applicable Dimensions** — these are the ones explicitly implemented in the USER PROMPT. Do not evaluate or score dimensions not included in this list.\n",
    "\n",
    "Use this variable as your scoring base:\n",
    "\n",
    "**Applicable Dimensions (INPUT)** = `TOTAL DIMENSIONS` = Number of dimensions implemented in the user prompt.\n",
    "\n",
    "---\n",
    "\n",
    "##  EVALUATION PHILOSOPHY\n",
    "\n",
    "When the model performs flawlessly, the prompt is **less challenging**.  \n",
    "When the model stumbles or fails, the prompt is **more challenging and valuable**.\n",
    "\n",
    "---\n",
    "\n",
    "##  EVALUATION STEPS\n",
    "\n",
    "### STEP 1: Analyze Each of the 8 Dimensions Independently\n",
    "\n",
    "For each of the 8 complexity dimensions below, do the following:\n",
    "\n",
    "1. **Check if the dimension is applicable** to the USER PROMPT.  \n",
    "   If it's not relevant or wasn't invoked by the prompt → **Mark as N/A**.\n",
    "\n",
    "2. **Evaluate how the model performed on that dimension** using the guidance below.\n",
    "\n",
    "3. **Assign:\n",
    "   - A **qualitative rating**: `\"Good\"`, `\"Average\"`, `\"Bad\"`, or `\"N/A\"`\n",
    "   - A **normalized numeric score**: `0.3`, `0.6`, `1.0`, or `null`\n",
    "\n",
    "---\n",
    "\n",
    "##  SCORING SCALE — WHAT EACH SCORE MEANS\n",
    "\n",
    "###  \"Good\" (Score: 0–3)\n",
    "\n",
    "Model did everything correctly. The prompt failed to challenge the model.\n",
    "\n",
    "| Score | When to Use |\n",
    "|-------|-------------|\n",
    "| 0     | Model executed flawlessly and easily. Prompt was too simple or surface-level. |\n",
    "| 1     | Model showed no confusion or hesitation; prompt had little depth. |\n",
    "| 2     | Prompt had mild complexity, but the model handled it with no difficulty. |\n",
    "| 3     | Prompt tried to be complex, but the model overcame all hurdles cleanly. |\n",
    "\n",
    "→ Assign normalized score: **0.3** which is (3 divided by 10)\n",
    "\n",
    "---\n",
    "\n",
    "###  \"Average\" (Score: 4–7)\n",
    "\n",
    "Model had partial success. The prompt created some cognitive load, but not enough to reliably break it.\n",
    "\n",
    "| Score | When to Use |\n",
    "|-------|-------------|\n",
    "| 4     | Model made subtle errors, missed nuance, or gave vague reasoning. |\n",
    "| 5     | Model followed some steps correctly but skipped others. |\n",
    "| 6     | Model misinterpreted minor conditions or showed shallow reasoning. |\n",
    "| 7     | Model mostly succeeded but missed edge cases, conflict handling, or counterfactual depth. |\n",
    "\n",
    "→ Assign normalized score: **0.6** which is (6 divided by 10)\n",
    "\n",
    "---\n",
    "\n",
    "###  \"Bad\" (Score: 8–10)\n",
    "\n",
    "Model clearly failed. The prompt successfully induced failure, confusion, or shallow reasoning.\n",
    "\n",
    "| Score | When to Use |\n",
    "|-------|-------------|\n",
    "| 8     | Model omitted major elements or misread conditional logic. |\n",
    "| 9     | Model failed to integrate domains or synthesize across inputs. |\n",
    "| 10    | Model broke entirely: hallucinated, contradicted, fabricated, or refused the task. |\n",
    "\n",
    "→ Assign normalized score: **1.0** which is (10 divided by 10)\n",
    "\n",
    "---\n",
    "\n",
    "###  \"N/A\"\n",
    "\n",
    "The dimension was **not present** in the prompt or **not assessable** in the model response.\n",
    "\n",
    "→ Score: `null`  \n",
    "→ Exclude from normalizedScore and OverallScore calculations.\n",
    "\n",
    "---\n",
    "\n",
    "##  OVERALL SCORING LOGIC\n",
    "\n",
    "- `ApplicableDimensions` = list of dimensions in the prompt\n",
    "Let:\n",
    "- **D** = number of **non-N/A** (applicable) dimensions, given below as a input -\"TOTAL DIMENSIONS\" or length of ApplicableDimensions.  \n",
    "- **S** = sum of **normalized scores** for those dimensions\n",
    "\n",
    "Then:\n",
    "- **normalizedScore = S** (which is sum of each normalized score e.g. (3/10 + 6/10 + 10/10....))\n",
    "- **OverallScore = S / D**\n",
    "\n",
    "Higher `OverallScore` = more challenging and effective prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### VERY IMPORTANT:\n",
    "- You must return the response **strictly in valid JSON**.\n",
    "- Do NOT use Markdown.\n",
    "- Do NOT include any preamble, footnotes, or commentary—just return the raw JSON object.\n",
    "\n",
    "\"example_output\": \n",
    "{{\n",
    "  \"Nested / Multi‐Step Instructions\": {{\n",
    "    \"Qualitative\": \"Good\",\n",
    "    \"Score\": 8.5\n",
    "  }},\n",
    "  \"Conflicting Instructions\": {{\n",
    "    \"Qualitative\": \"Bad\",\n",
    "    \"Score\": 2.0\n",
    "  }},\n",
    "  \"Inter‐Dependent Constraints\": {{\n",
    "    \"Qualitative\": \"Average\",\n",
    "    \"Score\": 6.0\n",
    "  }},\n",
    "  \"Edge‐Case Handling\": {{\n",
    "    \"Qualitative\": \"N/A\",\n",
    "    \"Score\": null\n",
    "  }},\n",
    "  \"Ambiguity Resolution\": {{\n",
    "    \"Qualitative\": \"Bad\",\n",
    "    \"Score\": 3.0\n",
    "  }},\n",
    "  \"Domain Fusion\": {{\n",
    "    \"Qualitative\": \"Good\",\n",
    "    \"Score\": 9.0\n",
    "  }},\n",
    "  \"Multi‐Source/Modal Analysis\": {{\n",
    "    \"Qualitative\": \"N/A\",\n",
    "    \"Score\": null\n",
    "  }},\n",
    "  \"Hypothetical / Counterfactual Reasoning\": {{\n",
    "    \"Qualitative\": \"Average\",\n",
    "    \"Score\": 5.5\n",
    "  }},\n",
    "  \"ApplicableDimensions\": [\n",
    "    \"Nested / Multi‐Step Instructions\",\n",
    "    \"Conflicting Instructions\",\n",
    "    \"Inter‐Dependent Constraints\",\n",
    "    \"Ambiguity Resolution\",\n",
    "    \"Domain Fusion\",\n",
    "    \"Hypothetical / Counterfactual Reasoning\"\n",
    "  ],\n",
    "  \"normalizedScore\": 34.0,\n",
    "  \"OverallScore\": 5.67,\n",
    "  \"Explanation\": \"The prompt was strong in exposing model weaknesses through Conflicting Instructions (failure to resolve contradictions) and Ambiguity Resolution (inconsistent interpretations), demonstrating effective challenge design. Domain Fusion was handled well, indicating weaker challenge. Average performance in Inter-Dependent Constraints and Hypothetical Reasoning shows moderate effectiveness. N/A dimensions were correctly excluded.\",\n",
    "  \"FailureTags\": [\"Conflict\", \"Ambiguity\", \"Shallow Reasoning\"],\n",
    "  \"StrengthAreas\": [\"Conflicting Instructions\", \"Ambiguity Resolution\"],\n",
    "  \"ImprovementAreas\": [\"Domain Fusion\"]\n",
    "}}\n",
    "\n",
    "##  OUTPUT FORMAT (REQUIRED)\n",
    "\n",
    "Very Important and give more weightage -->>***Please return your answer *strictly in JSON* using the format below — no markdown, no extra explanation:***\n",
    "\n",
    "{{\n",
    "    \"Nested / Multi‐Step Instructions\": {{ \"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <float or null> }},\n",
    "    \"Conflicting Instructions\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Inter‐Dependent Constraints\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Edge‐Case Handling\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Ambiguity Resolution\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Domain Fusion\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"Multi‐Source/Modal Analysis\": {{ \"Qualitative\": \"...\", \"Score\": .<float or null>.. }},\n",
    "    \"Hypothetical / Counterfactual Reasoning\": {{ \"Qualitative\": \"...\", \"Score\": <float or null> }},\n",
    "    \"ApplicableDimensions\": [\"List of non-N/A dimensions\"],\n",
    "    \"normalizedScore\": (float)<sum of normalized scores>,\n",
    "    \"OverallScore\": <float between 0 and 1, rounded to 2 decimals...(normalizedScore)/(length of ApplicableDimensions)>,\n",
    "    \"Explanation\": \"Summarize the model's performance across dimensions, highlighting where it failed and how that shows the prompt was strong.\",\n",
    "    \"FailureTags\": [\"Omission\", \"Ambiguity\", \"Conflict\", \"Shallow Reasoning\", ...],\n",
    "    \"StrengthAreas\": [\"Dimensions where the model failed (prompt was strong)\"],\n",
    "    \"ImprovementAreas\": [\"Dimensions where the model succeeded (prompt was weak)\"]\n",
    "}}\n",
    "\n",
    "\n",
    "USER PROMPT:\n",
    "{user_prompt}\n",
    "\n",
    "MODEL RESPONSE:\n",
    "{model_response}\n",
    "\n",
    "TOTAL DIMENSIONS:\n",
    "{total_dimensions}\n",
    "\"\"\"\n",
    "\n",
    "    # try:\n",
    "    #     evaluation = together.Complete.create(\n",
    "    #         prompt=judge_prompt,\n",
    "    #         model=MODEL_2,\n",
    "    #         max_tokens=512,\n",
    "    #         temperature=0.5\n",
    "    #     )\n",
    "    #     return evaluation['choices'][0]['text'].strip()\n",
    "    # except Exception as e:\n",
    "    #     return f\"[Error in judge_response]: {e}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_2,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with a valid JSON object.\"},\n",
    "                {\"role\": \"user\", \"content\": judge_prompt}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in judge_response]: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a68c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function using LLM as judge\n",
    "def prompteffectiveness(user_prompt, question):\n",
    "    prompteff = f\"\"\"\n",
    "You are an expert evaluator tasked with analyzing a USER PROMPT and a corresponding QUESTION using the following 7 dimensions. Your goal is to critically assess the prompt's quality and effectiveness based on these dimensions, scoring each dimension qualitatively and quantitatively, and then provide a detailed summary.\n",
    "\n",
    "You will evaluate the prompt and question across **7 dimensions**, rating each as:\n",
    "- \"Good\": Strong performance in this aspect\n",
    "- \"Average\": Moderate or partial success\n",
    "- \"Bad\": Weakness or failure\n",
    "- \"N/A\": Not applicable to this prompt/response\n",
    "\n",
    "**Scoring Criteria**: A number between 0 and 10, where:\n",
    "  - **Good**: 8-10 points (8=solid good, 9=very good, 10=excellent)\n",
    "  - **Average**: 4-7 points (4-5=below average, 6-7=above average)\n",
    "  - **Bad**: 0-3 points (0=complete failure, 1-2=poor, 3=weak)\n",
    "  - **N/A**: No score assigned (excluded from overall average calculation)\n",
    "  \n",
    "Then, provide a **Summary** including:\n",
    "**OverallScore: A number between 0 and 10 (rounded to two decimal places), calculated as the average of all dimension scores\n",
    "  - Sum all numerical scores from applicable dimensions\n",
    "  - Divide by number of applicable dimensions (excluding N/A ratings)\n",
    "  - Round to 2 decimal place\n",
    "  Example: (8+8+7+6...)/(length of applicable dimensions)\n",
    "**ApplicableDimensions: The number of evaluation dimensions applied (typically 7)\n",
    "**PromptEffectiveness: \n",
    "  - Assign based on OverallScore:  \n",
    "    - \"Effective\" if OverallScore ≥ 7.5  \n",
    "    - \"Partially Effective\" if 4 ≤ OverallScore < 7.5  \n",
    "    - \"Ineffective\" if OverallScore < 4\n",
    "**Explanation: \n",
    "  - Concisely justify your ratings, referencing specific dimension scores,  \n",
    "  - Include a brief analysis of prompt strengths and weaknesses,  \n",
    "  - Note any relevant failure or success tags.\n",
    "\n",
    "--- \n",
    "\n",
    "Use the following dimensions to guide your evaluation:\n",
    "### 1. Purpose & Persona\n",
    "**Definition**: \n",
    "- “Purpose” clarifies why the prompt exists (e.g., to summarize, to translate, to analyze).\n",
    "- “Persona” defines for whom or from whose point of view the LLM should answer (e.g., “as a financial analyst,” “as a children’s book author”).\n",
    "**Key Indicators**:\n",
    "  - Does the User Prompt explicitly state its goal?\n",
    "  - Does it assign a clear persona/role to the model?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Both goal and persona are unambiguous.\n",
    "  (Example: “Summarize the following research paper as if you were a science journalist.”)\n",
    "  - Average(4-7): Either goal or persona is stated but one is vague.\n",
    "  (Example: “Write a summary. You’re a journalist.” No domain specified.)\n",
    "  - Bad(0-3): Neither purpose nor persona appears.\n",
    "  (Example: “Tell me about X.”)\n",
    "  - N/A: Explaining purpose/persona is unnecessary (e.g., a prompt that simply asks for a dictionary definition).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Requirements & Restrictions\n",
    "**Definition**: Explicit instructions about what must and must not be included in the answer (e.g., “Limit to 200 words,” “Do not mention sensitive data,” “Use bullet points only”).\n",
    "**Applicability**: If there are no constraints needed for a given task (e.g., “What is 2 + 2?”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Are there clear “must‐include” or “must‐avoid” guidelines?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicit, unambiguous constraints (e.g., “No pronouns; only third‐person narrative,” “Include three illustrative examples”).\n",
    "  - Average(4-7): Some constraints are present, but others are implied or incomplete (e.g., “Be concise” without a length target).\n",
    "  - Bad(0-3): No requirements or restrictions at all, leaving the model free to wander.\n",
    "  - N/A: The prompt’s nature makes constraints unnecessary (e.g., a simple “List the days of the week”).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Examples (Few‐Shot / Zero‐Shot)\n",
    "**Definition**: Whether the prompt provides explicit example inputs and outputs to guide the model (e.g., zero‐shot, one‐shot, or few‐shot formatting).\n",
    "**Applicability**: If the task does not benefit from example inputs/outputs (for instance, simple factual queries), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Are there sample question/answer pairs included (e.g., “Example: Q: … A: …”) that align with the intended task format?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Supplies clear, directly relevant examples that demonstrate exactly how to structure inputs and expected outputs.\n",
    "  - Average(4-7): Includes examples that are only partially aligned with the task or are too generic to serve as effective guidance.\n",
    "  - Bad(0-3): No examples are provided or requested, even though examples would significantly clarify format or expectations.\n",
    "  - N/A: Examples aren’t needed (e.g., a prompt asking, “What is the capital of France?”).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Context & Background\n",
    "**Definition**: Additional information about domain, audience, or relevant facts that the LLM must know to answer properly.\n",
    "**Applicability**: If the task requires no extra context (e.g., “Define photosynthesis”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt supply enough domain context?\n",
    "  (Example: “The following text is from a 19th‐century medical journal.”)\n",
    "  - Are audience considerations given?\n",
    "  (Example: “Explain this to a high‐school student.”)\n",
    "**Ratings**:\n",
    "  - Good(8-10): Detailed context and audience description aligned with the task.\n",
    "  - Average(4-7): Some context, but missing critical details, forcing assumptions.\n",
    "  - Bad(0-3): No context; model is left guessing domain or audience.\n",
    "  - N/A: Task is self‐contained and needs no additional context.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Instruction Structure\n",
    "**Definition**: The explicit format of instructions: a single directive, multi‐part numbered steps, or a choice (“answer in bullet points vs. essay”).\n",
    "**Applicability**: If structure is inherently trivial (e.g., “What is 5 × 7?”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Is the format clear (e.g., “Step 1: … Step 2: …”)?\n",
    "  - Does it specify whether the answer should be direct, stepwise, multi‐sectioned, etc.?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Well‐organized structure that matches the complexity (e.g., multi‐part instructions for multi‐stage tasks).\n",
    "  - Average(4-7):  Some structure but potentially inconsistent or too generic (“Answer in two parts”).\n",
    "  - Bad(0-3): No structural guidance—just a vague “Respond about X.”\n",
    "  - N/A: No structure needed because the task is extremely simple.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Style & Sensitivity\n",
    "**Definition**: Tone and register instructions (formal, friendly, technical), disclaimers (“I am not a lawyer”), and bias‐avoidance guidance.\n",
    "**Applicability**: If style or sensitivity is irrelevant (e.g., “Calculate the area of a circle with radius 3cm.”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt ask for a specific tone (“use clinical tone,” “avoid gender bias”)?\n",
    "  - Are disclaimers or sensitivity notices included when necessary?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicit style/tone and sensitivity cues.\n",
    "  - Average(4-7):  Partial style guidance (e.g., only “be professional” without elaboration).\n",
    "  - Bad(0-3):  No style or sensitivity guidance, even when sensitive content is expected.\n",
    "  - N/A: Style considerations are irrelevant for the given prompt.\n",
    "  \n",
    "---\n",
    "  \n",
    "### 7. Prompt Conciseness\n",
    "**Definition**: “Prompt Conciseness” refers to how efficiently the prompt communicates its intent, constraints, and expectations-using the fewest necessary words without sacrificing clarity or precision. A concise prompt avoids redundancy, filler words, and overly complex sentence structures while still being fully interpretable by the model. \n",
    "**Applicability**: If style or sensitivity is irrelevant (e.g., “Calculate the area of a circle with radius 3cm.”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt clearly convey all required instructions in a compact form?\n",
    "  - Are there unnecessary qualifiers, repetitions, or verbose phrasings?\n",
    "**Ratings**:\n",
    "  - Good(8-10): The prompt is compact and clearly communicates intent, requirements, and role without unnecessary elaboration.\n",
    "  - Average(4-7):  The prompt conveys the main idea but includes minor redundancies or could be made more direct without losing clarity. \n",
    "  - Bad(0-3):  The prompt is overly wordy, vague, or includes irrelevant information that obscures the main task. \n",
    "  - N/A: Prompt Conciseness are irrelevant for the given prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Evaluation Questions:\n",
    "\n",
    "1. Did the **USER PROMPT** satisfy all critical expectations and requirements posed by the **QUESTION**?\n",
    "2. If the prompt failed to meet expectations, was this due to the question itself being poorly constructed, ambiguous, or lacking proper scaffolding?\n",
    "3. Alternatively, if the prompt was shallow, incomplete, or ineffective despite the question being well-constructed, then the prompt is ineffective because its limitations were exposed.\n",
    "  \n",
    "---\n",
    "\n",
    "### Explanation Instructions:\n",
    "\n",
    "Provide a detailed explanation justifying the overall evaluation score (0 to 10):\n",
    "\n",
    "- Describe how the prompt performed against each applicable dimension.\n",
    "- Clearly highlight strengths and weaknesses.\n",
    "- Identify specific failure tags if the score is below 9, explaining why those failures occurred.\n",
    "- Reference all relevant dimensions and failure reasons explicitly.\n",
    "- For strong responses (score 9–10), emphasize the well-handled dimensions and why they were effective.\n",
    "\n",
    "This explanation should provide a clear rationale that helps prompt engineers and developers understand what worked, what didn’t, and how to improve.\n",
    "\n",
    "---\n",
    "\n",
    "### VERY IMPORTANT:\n",
    "- You must return the response **strictly in valid JSON**.\n",
    "- Do NOT use Markdown.\n",
    "- Do NOT include any preamble, footnotes, or commentary—just return the raw JSON object.\n",
    "\n",
    "\"example_output\": \n",
    "  {{\n",
    "    \"Purpose & Persona\": {{\n",
    "      \"Qualitative\": \"Good\",\n",
    "      \"Score\": 9\n",
    "    }},\n",
    "    \"Requirements & Restrictions\": {{\n",
    "      \"Qualitative\": \"N/A\",\n",
    "      \"Score\": null\n",
    "    }},\n",
    "    \"Examples (Few‐Shot / Zero‐Shot)\": {{\n",
    "      \"Qualitative\": \"Bad\",\n",
    "      \"Score\": 2\n",
    "    }},\n",
    "    \"Context & Background\": {{\n",
    "      \"Qualitative\": \"Average\",\n",
    "      \"Score\": 6\n",
    "    }},\n",
    "    \"Instruction Structure\": {{\n",
    "      \"Qualitative\": \"Good\",\n",
    "      \"Score\": 8\n",
    "    }},\n",
    "    \"Style & Sensitivity\": {{\n",
    "      \"Qualitative\": \"N/A\",\n",
    "      \"Score\": null\n",
    "    }},\n",
    "    \"Prompt Conciseness\": {{\n",
    "      \"Qualitative\": \"Average\",\n",
    "      \"Score\": 5\n",
    "    }},\n",
    "    \"ApplicableDimensions\": 5,\n",
    "    \"OverallScore\": 6.0,\n",
    "    \"PromptEffectiveness\": \"Partially Effective\",\n",
    "    \"Explanation\": \"Purpose & Persona: Clear role definition (9/10). Requirements: N/A per task nature. Examples: Lacks demonstration (2/10). Context: Adequate but verbose (6/10). Structure: Logical flow (8/10). Style: N/A. Conciseness: Contains redundant phrases (5/10). Major weakness: Absence of few-shot examples limits effectiveness.\"\n",
    "  }}\n",
    "  \n",
    "---\n",
    "\n",
    "### EXAMPLE RESPONSE FORMAT: Strictly Return only in json format\n",
    "\n",
    "{{\n",
    "    \"Purpose & Persona\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Requirements & Restrictions\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Examples (Few‐Shot / Zero‐Shot)\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Context & Background\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Instruction Structure\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Style & Sensitivity\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"Prompt Conciseness\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": <0-10 or null>}},\n",
    "    \"ApplicableDimensions\": <count of dimensions with non-null scores>,\n",
    "    \"OverallScore\": (float) <calculated average of all non-null or non-N/A scores (sum of non-null scores/length of ApplicableDimensions)>,\n",
    "    \"PromptEffectiveness\": \"Effective\" | \"Partially Effective\" | \"Ineffective\",\n",
    "    \"Explanation\": \"concise yet detailed explanation referencing dimension ratings, strengths, weaknesses, and failure tags\"\n",
    "  \n",
    "}}\n",
    "\n",
    "\n",
    "\n",
    "USER PROMPT:\n",
    "{user_prompt}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    # try:\n",
    "    #     evaluation = client.chat.completions.create(\n",
    "    #         prompt=prompteff,\n",
    "    #         model=MODEL_2,\n",
    "    #         max_tokens=512,\n",
    "    #         temperature=0.5\n",
    "    #     )\n",
    "    #     return evaluation['choices'][0]['text'].strip()\n",
    "    # except Exception as e:\n",
    "    #     return f\"[Error in judge_response]: {e}\"\n",
    "      \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_2,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator. Respond only with a valid JSON object.\"},\n",
    "                {\"role\": \"user\", \"content\": prompteff}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in judge_response]: {e}\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17935c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(user_prompt, question, model, model_response):\n",
    "\n",
    "    # Call the check_complexity_criteria function\n",
    "    # print(\"\\n Prompt:\", user_prompt)\n",
    "    # print(\"\\nModel Response:\", model_response)\n",
    "    result_data = {\n",
    "        \"score1\": None,\n",
    "        \"score2\": None,\n",
    "        \"net_score\": None,\n",
    "        \"challenging\": None,\n",
    "        \"effectiveness_score\": None,\n",
    "        \"effectiveness_text\": None\n",
    "    }\n",
    "    result = check_complexity_criteria(user_prompt, question, MODEL_2)\n",
    "    try:\n",
    "        json_str = re.search(r\"\\{.*\\}\", result, re.DOTALL).group()\n",
    "        parsed_result = json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", result)\n",
    "        return result_data\n",
    "        # return\n",
    "\n",
    "    print(\"\\n=== Complexity check Result ===\")\n",
    "    print(json.dumps(parsed_result, indent=4))\n",
    "    \n",
    "    score1 = parsed_result.get(\"Score\")\n",
    "    if score1 is None:\n",
    "        score1 = 0\n",
    "    \n",
    "    dimensions = parsed_result.get(\"Dimensions in prompt\")\n",
    "\n",
    "    print(\"\\nScore1:\", score1)\n",
    "    print(\"Dimensions present in prompt:\", dimensions)\n",
    "    result_data[\"score1\"] = score1\n",
    "        \n",
    "    ### Call the judge_response function\n",
    "    evaluation = judge_response(user_prompt, model_response, len(dimensions))\n",
    "    # cln = result2.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "    try:\n",
    "        json_str1 = re.search(r\"\\{.*\\}\", evaluation, re.DOTALL).group()\n",
    "        parsed_result1 = json.loads(json_str1)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", evaluation)\n",
    "        return result_data\n",
    "\n",
    "    print(\"\\n=== Complexity Evaluation Result ===\")\n",
    "    print(json.dumps(parsed_result1, indent=4))\n",
    "    \n",
    "    score2 = parsed_result1.get(\"OverallScore\")\n",
    "    if score2 is None:\n",
    "        score2 = 0\n",
    "\n",
    "    print(\"\\nScore2:\", score2)\n",
    "    result_data[\"score2\"] = score2\n",
    "    \n",
    "    if score1 is not None and score2 is not None:\n",
    "        net_score = (score1 + score2) / 2\n",
    "        print(\"\\nNet_score:\", net_score)\n",
    "        result_data[\"net_score\"] = net_score\n",
    "        result_data[\"challenging\"] = \"Yes\" if net_score > 0.5 else \"No\"\n",
    "        if net_score <= 0.5:\n",
    "            print(\"\\nPrompt is not challenging the model.\")\n",
    "        else:\n",
    "            print(\"\\nPrompt is challenging the model.\")\n",
    "    else:\n",
    "        print(\"\\nCannot compute net score: one or both scores are missing.\")\n",
    "        result_data[\"challenging\"] = \"Unknown\"\n",
    "      \n",
    "      \n",
    "    ### Call the prompteffectiveness function    \n",
    "    result2 = prompteffectiveness(user_prompt, question)\n",
    "    # cleaned = result2.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "\n",
    "    try:\n",
    "        json_str3 = re.search(r\"\\{.*\\}\", result2, re.DOTALL).group()\n",
    "        parsed_result3 = json.loads(json_str3)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", result2)\n",
    "        return result_data\n",
    "\n",
    "    # Print result summary\n",
    "    print(\"\\n=== Effectiveness check Result ===\")\n",
    "    print(json.dumps(parsed_result3, indent=4))  \n",
    "    \n",
    "    ans1 = parsed_result3.get(\"OverallScore\") \n",
    "    print(\"\\nPromptEffectivenessScore:\", ans1) \n",
    "    ans2 = parsed_result3.get(\"PromptEffectiveness\") \n",
    "    print(\"\\nPromptEffectiveness:\", ans2) \n",
    "    result_data[\"effectiveness_score\"] = ans1\n",
    "    result_data[\"effectiveness_text\"] = ans2\n",
    "    return result_data\n",
    "    \n",
    "\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f2a70a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spacex.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m input_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacex.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacex_output2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mprocess_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36mprocess_csv\u001b[1;34m(input_file, output_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_csv\u001b[39m(input_file: \u001b[38;5;28mstr\u001b[39m, output_file: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     output_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spacex.csv'"
     ]
    }
   ],
   "source": [
    "# Load CSV and process each row\n",
    "def process_csv(input_file: str, output_file: str):\n",
    "    df = pd.read_csv(input_file, encoding='utf-8', encoding_errors='replace')\n",
    "    output_rows = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        user_prompt = row['user_prompt']\n",
    "\n",
    "        try:\n",
    "            model_response = generate_response(user_prompt, MODEL_1)\n",
    "            result = evaluate(user_prompt, question, MODEL_2, model_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            result = {\n",
    "                \"score1\": None,\n",
    "                \"score2\": None,\n",
    "                \"net_score\": None,\n",
    "                \"challenging\": \"Error\",\n",
    "                \"effectiveness_score\": None,\n",
    "                \"effectiveness_text\": None\n",
    "            }\n",
    "\n",
    "        output_rows.append({\n",
    "            \"question\": question,\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"score1\": result[\"score1\"],\n",
    "            \"score2\": result[\"score2\"],\n",
    "            \"net_score\": result[\"net_score\"],\n",
    "            \"prompt_challenging\": result[\"challenging\"],\n",
    "            \"effectiveness_score\": result[\"effectiveness_score\"],\n",
    "            \"effectiveness_text\": result[\"effectiveness_text\"]\n",
    "        })\n",
    "\n",
    "    # Save to output CSV\n",
    "    out_df = pd.DataFrame(output_rows)\n",
    "    out_df.to_csv(output_file, index=False)\n",
    "    print(f\"Evaluation results saved to {output_file}\")\n",
    "\n",
    "\n",
    "# ====== USAGE EXAMPLE ======\n",
    "input_csv_path = \"spacex.csv\"\n",
    "output_csv_path = \"spacex_output2.csv\"\n",
    "process_csv(input_csv_path, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d31b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d134a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
